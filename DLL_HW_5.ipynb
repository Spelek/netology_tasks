{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашняя работа №5. Рекуррентные сети"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def caesar(plaintext, shift):\n",
    "    alphabet = 'абвгдеёжзийклмнопрстуфхцчшщъыьэюя'\n",
    "    shifted_alphabet = alphabet[shift:] + alphabet[:shift]\n",
    "    table = str.maketrans(alphabet, shifted_alphabet)\n",
    "    return plaintext.translate(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from random import randint\n",
    "\n",
    "data = []\n",
    "encoded = []\n",
    "encoded_rand = []\n",
    "SIGNS = set('IVXL')\n",
    "\n",
    "with open('onegin.txt', mode='r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        if (len(line.strip()) < 8) and (line[0] in SIGNS):\n",
    "            f.readline()\n",
    "            for _ in range(14):\n",
    "                sent = ' '.join(re.findall(r'[а-яё]+', f.readline().lower()))\n",
    "                if len(sent) == 0:\n",
    "                    break\n",
    "                data.append(sent)\n",
    "                encoded.append(caesar(sent, 2))\n",
    "                encoded_rand.append(caesar(sent, randint(2, 5)))\n",
    "        if len(data) >= 14 * 10:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('мой дядя самых честных правил', 'нпк еаеа тбньц шётуоьц рсбгйм')"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0], caesar(data[0], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHARS = set('абвгдеёжзийклмнопрстуфхцчшщъыьэюя ')\n",
    "INDEX_TO_CHAR = ['none'] + [w for w in CHARS]\n",
    "CHAR_TO_INDEX = {w: i for i, w in enumerate(INDEX_TO_CHAR)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(INDEX_TO_CHAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(max(data, key=lambda x: len(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN = 40\n",
    "X = torch.zeros((len(data), MAX_LEN), dtype=int)\n",
    "XR = torch.zeros((len(data), MAX_LEN), dtype=int)\n",
    "Y = torch.zeros((len(data), MAX_LEN), dtype=int)\n",
    "for i in range(len(data)):\n",
    "    for j, w in enumerate(data[i]):\n",
    "        if j >= MAX_LEN:\n",
    "            break\n",
    "        X[i, j] = CHAR_TO_INDEX.get(encoded[i][j], CHAR_TO_INDEX['none'])\n",
    "        XR[i, j] = CHAR_TO_INDEX.get(encoded_rand[i][j], CHAR_TO_INDEX['none'])\n",
    "        Y[i, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX['none'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 3, 17, 34,  7,  1, 33,  1, 33,  7, 32, 12,  3, 24, 14,  7, 16, 29, 32,\n",
       "         8, 15, 24, 14,  7, 23, 19, 12, 21,  6, 31,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание №1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Модель RNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Network, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(len(CHAR_TO_INDEX), 35)\n",
    "        self.rnn = torch.nn.RNN(35, 128, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(128, len(INDEX_TO_CHAR))\n",
    "        \n",
    "    def forward(self, sentences, state=None):\n",
    "        embed = self.embedding(sentences)\n",
    "        o, s = self.rnn(embed)\n",
    "        out = self.linear(o)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-03-12T15:20:53.260599Z",
     "start_time": "2020-03-12T15:20:53.256979Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Network()\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, num_epochs, data, target):\n",
    "    N = int(len(data) * 0.8)\n",
    "    train, test = data[: N], data[N: ]\n",
    "    y_train, y_test = target[: N], target[N: ]\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    trainer = torch.optim.SGD(model.parameters(), lr=.05)\n",
    "\n",
    "    for ep in range(num_epochs):\n",
    "        \n",
    "        train_iters, train_passed  = 0, 0\n",
    "        train_loss, train_acc = 0., 0.\n",
    "        start=time.time()\n",
    "        \n",
    "        model.train()\n",
    "        for i in range(int(len(train) / 14)):\n",
    "            X_batch = train[i * 14:(i + 1) * 14]\n",
    "            Y_batch = y_train[i * 14:(i + 1) * 14].flatten()\n",
    "            trainer.zero_grad()\n",
    "            y_pred = model.forward(X_batch)\n",
    "            y_pred = y_pred.view(-1, len(INDEX_TO_CHAR))\n",
    "            l = loss(y_pred, Y_batch)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            train_loss += l.item()\n",
    "            train_acc += (y_pred.argmax(dim=1) == Y_batch).sum().item()\n",
    "            train_iters += 1\n",
    "            train_passed += Y_batch.shape[0]\n",
    "        \n",
    "        test_iters, test_passed  = 0, 0\n",
    "        test_loss, test_acc = 0., 0.\n",
    "        for i in range(int(len(test) / 14)):\n",
    "            X_batch = test[i * 14:(i + 1) * 14]\n",
    "            Y_batch = y_test[i * 14:(i + 1) * 14].flatten()\n",
    "            y_pred = model(X_batch).view(-1, len(INDEX_TO_CHAR))\n",
    "            l = loss(y_pred, Y_batch)\n",
    "            test_loss += l.item()\n",
    "            test_acc += (y_pred.argmax(dim=1) == Y_batch).sum().item()\n",
    "            test_iters += 1\n",
    "            test_passed += Y_batch.shape[0]\n",
    "            \n",
    "        train_acc_res = train_acc / train_passed\n",
    "        test_acc_res = test_acc / test_passed\n",
    "        print(\"ep: {}, time: {:.3f}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
    "            ep, time.time() - start, train_loss / train_iters, train_acc_res,\n",
    "            test_loss / test_iters, test_acc_res)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вариант №1. Смещение на 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, time: 0.080, train_loss: 2.9002789855003357, train_acc: 0.34732142857142856, test_loss: 2.1752326488494873, test_acc: 0.475\n",
      "ep: 1, time: 0.086, train_loss: 2.0248415619134903, train_acc: 0.5294642857142857, test_loss: 1.9276302456855774, test_acc: 0.6151785714285715\n",
      "ep: 2, time: 0.071, train_loss: 1.8318042308092117, train_acc: 0.6944196428571429, test_loss: 1.7724168300628662, test_acc: 0.7535714285714286\n",
      "ep: 3, time: 0.078, train_loss: 1.6758822798728943, train_acc: 0.8033482142857142, test_loss: 1.6282222867012024, test_acc: 0.8133928571428571\n",
      "ep: 4, time: 0.073, train_loss: 1.5291919857263565, train_acc: 0.8399553571428572, test_loss: 1.492543339729309, test_acc: 0.8330357142857143\n",
      "ep: 5, time: 0.058, train_loss: 1.3935012519359589, train_acc: 0.8535714285714285, test_loss: 1.3683003783226013, test_acc: 0.8419642857142857\n",
      "ep: 6, time: 0.066, train_loss: 1.2719790488481522, train_acc: 0.8569196428571428, test_loss: 1.2569966316223145, test_acc: 0.84375\n",
      "ep: 7, time: 0.081, train_loss: 1.1650408655405045, train_acc: 0.8613839285714285, test_loss: 1.1579590439796448, test_acc: 0.8455357142857143\n",
      "ep: 8, time: 0.096, train_loss: 1.0711594372987747, train_acc: 0.8671875, test_loss: 1.069740653038025, test_acc: 0.8571428571428571\n",
      "ep: 9, time: 0.090, train_loss: 0.9884018823504448, train_acc: 0.8752232142857143, test_loss: 0.9908140897750854, test_acc: 0.8705357142857143\n",
      "ep: 10, time: 0.091, train_loss: 0.9149523302912712, train_acc: 0.8919642857142858, test_loss: 0.9197524785995483, test_acc: 0.8991071428571429\n",
      "ep: 11, time: 0.088, train_loss: 0.8492404595017433, train_acc: 0.9095982142857143, test_loss: 0.8553414642810822, test_acc: 0.9080357142857143\n",
      "ep: 12, time: 0.078, train_loss: 0.7899999096989632, train_acc: 0.9223214285714286, test_loss: 0.7966236770153046, test_acc: 0.9142857142857143\n",
      "ep: 13, time: 0.083, train_loss: 0.7362607643008232, train_acc: 0.9305803571428571, test_loss: 0.7428738176822662, test_acc: 0.925\n",
      "ep: 14, time: 0.081, train_loss: 0.6872953325510025, train_acc: 0.9352678571428571, test_loss: 0.693543016910553, test_acc: 0.9375\n",
      "ep: 15, time: 0.075, train_loss: 0.6425519287586212, train_acc: 0.9415178571428572, test_loss: 0.6482028663158417, test_acc: 0.95\n",
      "ep: 16, time: 0.077, train_loss: 0.6015975028276443, train_acc: 0.9493303571428572, test_loss: 0.6065037548542023, test_acc: 0.9526785714285714\n",
      "ep: 17, time: 0.080, train_loss: 0.5640764273703098, train_acc: 0.9515625, test_loss: 0.5681464374065399, test_acc: 0.9571428571428572\n",
      "ep: 18, time: 0.087, train_loss: 0.5296830385923386, train_acc: 0.9537946428571429, test_loss: 0.5328644812107086, test_acc: 0.9607142857142857\n",
      "ep: 19, time: 0.172, train_loss: 0.4981452152132988, train_acc: 0.9575892857142857, test_loss: 0.5004141926765442, test_acc: 0.9642857142857143\n",
      "ep: 20, time: 0.098, train_loss: 0.4692145362496376, train_acc: 0.9589285714285715, test_loss: 0.4705686569213867, test_acc: 0.9678571428571429\n",
      "ep: 21, time: 0.082, train_loss: 0.4426615461707115, train_acc: 0.9607142857142857, test_loss: 0.44311507046222687, test_acc: 0.9678571428571429\n",
      "ep: 22, time: 0.074, train_loss: 0.4182734191417694, train_acc: 0.9625, test_loss: 0.4178539663553238, test_acc: 0.9678571428571429\n",
      "ep: 23, time: 0.076, train_loss: 0.395853191614151, train_acc: 0.9640625, test_loss: 0.39459875226020813, test_acc: 0.9678571428571429\n",
      "ep: 24, time: 0.080, train_loss: 0.37521957606077194, train_acc: 0.9660714285714286, test_loss: 0.3731759488582611, test_acc: 0.9678571428571429\n",
      "ep: 25, time: 0.082, train_loss: 0.35620664060115814, train_acc: 0.9674107142857142, test_loss: 0.353425070643425, test_acc: 0.9678571428571429\n",
      "ep: 26, time: 0.084, train_loss: 0.33866335451602936, train_acc: 0.9698660714285714, test_loss: 0.33519844710826874, test_acc: 0.96875\n",
      "ep: 27, time: 0.069, train_loss: 0.3224529102444649, train_acc: 0.9705357142857143, test_loss: 0.318360835313797, test_acc: 0.9696428571428571\n",
      "ep: 28, time: 0.084, train_loss: 0.30745186284184456, train_acc: 0.9716517857142857, test_loss: 0.30278852581977844, test_acc: 0.9705357142857143\n",
      "ep: 29, time: 0.081, train_loss: 0.29354895651340485, train_acc: 0.971875, test_loss: 0.288368821144104, test_acc: 0.9714285714285714\n",
      "ep: 30, time: 0.074, train_loss: 0.28064422123134136, train_acc: 0.9723214285714286, test_loss: 0.2749995142221451, test_acc: 0.9714285714285714\n",
      "ep: 31, time: 0.086, train_loss: 0.26864770986139774, train_acc: 0.9725446428571428, test_loss: 0.26258765161037445, test_acc: 0.9732142857142857\n",
      "ep: 32, time: 0.079, train_loss: 0.2574786450713873, train_acc: 0.9729910714285714, test_loss: 0.25104914605617523, test_acc: 0.9758928571428571\n",
      "ep: 33, time: 0.072, train_loss: 0.24706444330513477, train_acc: 0.9745535714285715, test_loss: 0.24030783772468567, test_acc: 0.9785714285714285\n",
      "ep: 34, time: 0.061, train_loss: 0.2373397834599018, train_acc: 0.9770089285714286, test_loss: 0.23029479384422302, test_acc: 0.98125\n",
      "ep: 35, time: 0.064, train_loss: 0.22824584878981113, train_acc: 0.978125, test_loss: 0.22094770520925522, test_acc: 0.9821428571428571\n",
      "ep: 36, time: 0.069, train_loss: 0.21972964704036713, train_acc: 0.9785714285714285, test_loss: 0.2122102603316307, test_acc: 0.9848214285714286\n",
      "ep: 37, time: 0.070, train_loss: 0.2117434199899435, train_acc: 0.9790178571428572, test_loss: 0.2040315344929695, test_acc: 0.9857142857142858\n",
      "ep: 38, time: 0.074, train_loss: 0.20424400456249714, train_acc: 0.9794642857142857, test_loss: 0.19636552035808563, test_acc: 0.9857142857142858\n",
      "ep: 39, time: 0.067, train_loss: 0.19719239696860313, train_acc: 0.9810267857142857, test_loss: 0.18917058408260345, test_acc: 0.9875\n",
      "ep: 40, time: 0.057, train_loss: 0.190553305670619, train_acc: 0.9819196428571428, test_loss: 0.18240894377231598, test_acc: 0.9892857142857143\n",
      "ep: 41, time: 0.065, train_loss: 0.18429472111165524, train_acc: 0.9832589285714286, test_loss: 0.1760464683175087, test_acc: 0.9892857142857143\n",
      "ep: 42, time: 0.070, train_loss: 0.17838765121996403, train_acc: 0.9837053571428571, test_loss: 0.1700521782040596, test_acc: 0.9901785714285715\n",
      "ep: 43, time: 0.063, train_loss: 0.17280571535229683, train_acc: 0.9841517857142857, test_loss: 0.16439801454544067, test_acc: 0.9901785714285715\n",
      "ep: 44, time: 0.068, train_loss: 0.1675250083208084, train_acc: 0.9845982142857143, test_loss: 0.15905830264091492, test_acc: 0.9901785714285715\n",
      "ep: 45, time: 0.077, train_loss: 0.16252361796796322, train_acc: 0.9845982142857143, test_loss: 0.15401002019643784, test_acc: 0.9901785714285715\n",
      "ep: 46, time: 0.071, train_loss: 0.15778170712292194, train_acc: 0.9854910714285714, test_loss: 0.14923172444105148, test_acc: 0.9901785714285715\n",
      "ep: 47, time: 0.070, train_loss: 0.15328111872076988, train_acc: 0.9857142857142858, test_loss: 0.14470435678958893, test_acc: 0.9901785714285715\n",
      "ep: 48, time: 0.065, train_loss: 0.1490052156150341, train_acc: 0.9866071428571429, test_loss: 0.14041023701429367, test_acc: 0.9910714285714286\n",
      "ep: 49, time: 0.069, train_loss: 0.1449388973414898, train_acc: 0.9872767857142857, test_loss: 0.13633324950933456, test_acc: 0.9910714285714286\n",
      "ep: 50, time: 0.060, train_loss: 0.14106813818216324, train_acc: 0.9877232142857143, test_loss: 0.1324586197733879, test_acc: 0.9910714285714286\n",
      "ep: 51, time: 0.062, train_loss: 0.1373802237212658, train_acc: 0.9881696428571428, test_loss: 0.12877294793725014, test_acc: 0.9910714285714286\n",
      "ep: 52, time: 0.068, train_loss: 0.13386342581361532, train_acc: 0.9881696428571428, test_loss: 0.12526381388306618, test_acc: 0.9910714285714286\n",
      "ep: 53, time: 0.074, train_loss: 0.13050691969692707, train_acc: 0.9881696428571428, test_loss: 0.12191974744200706, test_acc: 0.9910714285714286\n",
      "ep: 54, time: 0.062, train_loss: 0.1273007793352008, train_acc: 0.9881696428571428, test_loss: 0.11873038858175278, test_acc: 0.9910714285714286\n",
      "ep: 55, time: 0.067, train_loss: 0.12423577066510916, train_acc: 0.9881696428571428, test_loss: 0.11568603664636612, test_acc: 0.9910714285714286\n",
      "ep: 56, time: 0.073, train_loss: 0.12130342423915863, train_acc: 0.9881696428571428, test_loss: 0.11277776584029198, test_acc: 0.9910714285714286\n",
      "ep: 57, time: 0.068, train_loss: 0.11849586106836796, train_acc: 0.9881696428571428, test_loss: 0.10999731346964836, test_acc: 0.9910714285714286\n",
      "ep: 58, time: 0.065, train_loss: 0.1158058075234294, train_acc: 0.9881696428571428, test_loss: 0.10733706131577492, test_acc: 0.9910714285714286\n",
      "ep: 59, time: 0.067, train_loss: 0.11322642583400011, train_acc: 0.9886160714285714, test_loss: 0.10478998348116875, test_acc: 0.9910714285714286\n",
      "ep: 60, time: 0.067, train_loss: 0.11075149662792683, train_acc: 0.9890625, test_loss: 0.10234948620200157, test_acc: 0.9910714285714286\n",
      "ep: 61, time: 0.081, train_loss: 0.10837509110569954, train_acc: 0.9895089285714286, test_loss: 0.10000954195857048, test_acc: 0.9910714285714286\n",
      "ep: 62, time: 0.060, train_loss: 0.10609175357967615, train_acc: 0.9897321428571428, test_loss: 0.09776432067155838, test_acc: 0.9919642857142857\n",
      "ep: 63, time: 0.067, train_loss: 0.10389639250934124, train_acc: 0.9899553571428571, test_loss: 0.09560860320925713, test_acc: 0.9919642857142857\n",
      "ep: 64, time: 0.061, train_loss: 0.10178423766046762, train_acc: 0.9901785714285715, test_loss: 0.09353752434253693, test_acc: 0.9919642857142857\n",
      "ep: 65, time: 0.064, train_loss: 0.09975076280534267, train_acc: 0.9901785714285715, test_loss: 0.091546431183815, test_acc: 0.9919642857142857\n",
      "ep: 66, time: 0.064, train_loss: 0.09779183007776737, train_acc: 0.9901785714285715, test_loss: 0.08963096886873245, test_acc: 0.9919642857142857\n",
      "ep: 67, time: 0.062, train_loss: 0.09590352047234774, train_acc: 0.9901785714285715, test_loss: 0.08778716623783112, test_acc: 0.9919642857142857\n",
      "ep: 68, time: 0.083, train_loss: 0.09408214036375284, train_acc: 0.9901785714285715, test_loss: 0.08601124957203865, test_acc: 0.9919642857142857\n",
      "ep: 69, time: 0.067, train_loss: 0.09232427086681128, train_acc: 0.990625, test_loss: 0.08429962396621704, test_acc: 0.9919642857142857\n",
      "ep: 70, time: 0.067, train_loss: 0.09062663093209267, train_acc: 0.9910714285714286, test_loss: 0.08264901861548424, test_acc: 0.9919642857142857\n",
      "ep: 71, time: 0.067, train_loss: 0.08898623380810022, train_acc: 0.9910714285714286, test_loss: 0.08105634152889252, test_acc: 0.9928571428571429\n",
      "ep: 72, time: 0.075, train_loss: 0.08740019146353006, train_acc: 0.9910714285714286, test_loss: 0.07951865345239639, test_acc: 0.9928571428571429\n",
      "ep: 73, time: 0.060, train_loss: 0.08586577419191599, train_acc: 0.9910714285714286, test_loss: 0.07803308218717575, test_acc: 0.9928571428571429\n",
      "ep: 74, time: 0.063, train_loss: 0.08438051678240299, train_acc: 0.9915178571428571, test_loss: 0.07659728080034256, test_acc: 0.99375\n",
      "ep: 75, time: 0.067, train_loss: 0.08294202573597431, train_acc: 0.9919642857142857, test_loss: 0.0752086341381073, test_acc: 0.9955357142857143\n",
      "ep: 76, time: 0.066, train_loss: 0.08154801279306412, train_acc: 0.9928571428571429, test_loss: 0.07386494427919388, test_acc: 0.9955357142857143\n",
      "ep: 77, time: 0.064, train_loss: 0.08019646070897579, train_acc: 0.9933035714285714, test_loss: 0.0725640133023262, test_acc: 0.9964285714285714\n",
      "ep: 78, time: 0.071, train_loss: 0.07888530008494854, train_acc: 0.9933035714285714, test_loss: 0.07130395621061325, test_acc: 0.9964285714285714\n",
      "ep: 79, time: 0.063, train_loss: 0.07761272881180048, train_acc: 0.9933035714285714, test_loss: 0.0700826533138752, test_acc: 0.9964285714285714\n",
      "ep: 80, time: 0.080, train_loss: 0.07637695455923676, train_acc: 0.9933035714285714, test_loss: 0.06889842078089714, test_acc: 0.9964285714285714\n",
      "ep: 81, time: 0.068, train_loss: 0.07517631957307458, train_acc: 0.9935267857142858, test_loss: 0.06774959340691566, test_acc: 0.9964285714285714\n",
      "ep: 82, time: 0.070, train_loss: 0.0740092913620174, train_acc: 0.99375, test_loss: 0.06663449108600616, test_acc: 0.9964285714285714\n",
      "ep: 83, time: 0.069, train_loss: 0.07287431927397847, train_acc: 0.9941964285714285, test_loss: 0.06555169820785522, test_acc: 0.9964285714285714\n",
      "ep: 84, time: 0.064, train_loss: 0.07177013205364347, train_acc: 0.9941964285714285, test_loss: 0.06449976935982704, test_acc: 0.9964285714285714\n",
      "ep: 85, time: 0.073, train_loss: 0.07069539511576295, train_acc: 0.9941964285714285, test_loss: 0.06347734108567238, test_acc: 0.9964285714285714\n",
      "ep: 86, time: 0.067, train_loss: 0.06964878411963582, train_acc: 0.9941964285714285, test_loss: 0.062483105808496475, test_acc: 0.9964285714285714\n",
      "ep: 87, time: 0.064, train_loss: 0.06862921500578523, train_acc: 0.9941964285714285, test_loss: 0.06151596084237099, test_acc: 0.9964285714285714\n",
      "ep: 88, time: 0.074, train_loss: 0.06763557577505708, train_acc: 0.9946428571428572, test_loss: 0.06057480908930302, test_acc: 0.9964285714285714\n",
      "ep: 89, time: 0.071, train_loss: 0.0666668307967484, train_acc: 0.9950892857142857, test_loss: 0.05965840443968773, test_acc: 0.9964285714285714\n",
      "ep: 90, time: 0.079, train_loss: 0.06572200497612357, train_acc: 0.9950892857142857, test_loss: 0.05876590125262737, test_acc: 0.9964285714285714\n",
      "ep: 91, time: 0.084, train_loss: 0.06480014882981777, train_acc: 0.9950892857142857, test_loss: 0.05789623036980629, test_acc: 0.9964285714285714\n",
      "ep: 92, time: 0.063, train_loss: 0.06390039902180433, train_acc: 0.9950892857142857, test_loss: 0.05704863555729389, test_acc: 0.9964285714285714\n",
      "ep: 93, time: 0.065, train_loss: 0.06302193459123373, train_acc: 0.9953125, test_loss: 0.05622217804193497, test_acc: 0.9964285714285714\n",
      "ep: 94, time: 0.063, train_loss: 0.06216400163248181, train_acc: 0.9955357142857143, test_loss: 0.05541601777076721, test_acc: 0.9964285714285714\n",
      "ep: 95, time: 0.069, train_loss: 0.06132583413273096, train_acc: 0.9955357142857143, test_loss: 0.054629502817988396, test_acc: 0.9964285714285714\n",
      "ep: 96, time: 0.067, train_loss: 0.060506708454340696, train_acc: 0.9955357142857143, test_loss: 0.05386170744895935, test_acc: 0.9964285714285714\n",
      "ep: 97, time: 0.071, train_loss: 0.05970596382394433, train_acc: 0.9955357142857143, test_loss: 0.05311213806271553, test_acc: 0.9964285714285714\n",
      "ep: 98, time: 0.073, train_loss: 0.05892300605773926, train_acc: 0.9957589285714286, test_loss: 0.05238007381558418, test_acc: 0.9964285714285714\n",
      "ep: 99, time: 0.075, train_loss: 0.058157240971922874, train_acc: 0.9957589285714286, test_loss: 0.05166485533118248, test_acc: 0.9964285714285714\n",
      "ep: 100, time: 0.066, train_loss: 0.05740808788686991, train_acc: 0.9959821428571428, test_loss: 0.05096595920622349, test_acc: 0.9964285714285714\n",
      "ep: 101, time: 0.063, train_loss: 0.05667496286332607, train_acc: 0.9964285714285714, test_loss: 0.050282880663871765, test_acc: 0.9964285714285714\n",
      "ep: 102, time: 0.085, train_loss: 0.05595744168385863, train_acc: 0.9964285714285714, test_loss: 0.0496149230748415, test_acc: 0.9973214285714286\n",
      "ep: 103, time: 0.061, train_loss: 0.05525494925677776, train_acc: 0.9964285714285714, test_loss: 0.04896163381636143, test_acc: 0.9982142857142857\n",
      "ep: 104, time: 0.068, train_loss: 0.05456709396094084, train_acc: 0.9966517857142857, test_loss: 0.04832264222204685, test_acc: 0.9982142857142857\n",
      "ep: 105, time: 0.070, train_loss: 0.053893403615802526, train_acc: 0.9966517857142857, test_loss: 0.047697313129901886, test_acc: 0.9982142857142857\n",
      "ep: 106, time: 0.071, train_loss: 0.05323343491181731, train_acc: 0.9966517857142857, test_loss: 0.04708538390696049, test_acc: 0.9982142857142857\n",
      "ep: 107, time: 0.061, train_loss: 0.05258684093132615, train_acc: 0.9966517857142857, test_loss: 0.046486372128129005, test_acc: 0.9982142857142857\n",
      "ep: 108, time: 0.065, train_loss: 0.05195320723578334, train_acc: 0.9966517857142857, test_loss: 0.04589988850057125, test_acc: 0.9982142857142857\n",
      "ep: 109, time: 0.073, train_loss: 0.051332182716578245, train_acc: 0.996875, test_loss: 0.0453255046159029, test_acc: 0.9991071428571429\n",
      "ep: 110, time: 0.057, train_loss: 0.05072340974584222, train_acc: 0.996875, test_loss: 0.04476288706064224, test_acc: 0.9991071428571429\n",
      "ep: 111, time: 0.067, train_loss: 0.050126454792916775, train_acc: 0.996875, test_loss: 0.04421166889369488, test_acc: 0.9991071428571429\n",
      "ep: 112, time: 0.073, train_loss: 0.049541184678673744, train_acc: 0.996875, test_loss: 0.04367160424590111, test_acc: 0.9991071428571429\n",
      "ep: 113, time: 0.063, train_loss: 0.04896716261282563, train_acc: 0.996875, test_loss: 0.0431422870606184, test_acc: 0.9991071428571429\n",
      "ep: 114, time: 0.062, train_loss: 0.04840415250509977, train_acc: 0.996875, test_loss: 0.04262353479862213, test_acc: 0.9991071428571429\n",
      "ep: 115, time: 0.069, train_loss: 0.04785184795036912, train_acc: 0.996875, test_loss: 0.0421148557215929, test_acc: 0.9991071428571429\n",
      "ep: 116, time: 0.069, train_loss: 0.04730995558202267, train_acc: 0.996875, test_loss: 0.0416161734610796, test_acc: 0.9991071428571429\n",
      "ep: 117, time: 0.064, train_loss: 0.04677823977544904, train_acc: 0.996875, test_loss: 0.04112707078456879, test_acc: 0.9991071428571429\n",
      "ep: 118, time: 0.065, train_loss: 0.046256398782134056, train_acc: 0.996875, test_loss: 0.04064738750457764, test_acc: 0.9991071428571429\n",
      "ep: 119, time: 0.067, train_loss: 0.0457442665938288, train_acc: 0.996875, test_loss: 0.04017682373523712, test_acc: 0.9991071428571429\n"
     ]
    }
   ],
   "source": [
    "model = Network()\n",
    "train_model(model, 120, X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(sent):\n",
    "    x = torch.zeros((1, len(sent)), dtype=int)\n",
    "    output = ''\n",
    "\n",
    "    for j, w in enumerate(sent):\n",
    "        x[0, j] = CHAR_TO_INDEX.get(w, CHAR_TO_INDEX['none'])\n",
    "    \n",
    "    o = model(x)[0]\n",
    "    for w in o:\n",
    "        ww = INDEX_TO_CHAR[torch.argmax(w)]        \n",
    "        output += ww\n",
    "    \n",
    "    return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Визуальная проверка результата"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: орл ёбёб увоэч щжуфпэч ствдкн\n",
      "original: мой дядя самых честных правил\n",
      "decoded: мой дядя самых честных правил\n",
      "=====\n",
      "encoded: мреёв пж д ъхфмх йвпжоре\n",
      "original: когда не в шутку занемог\n",
      "decoded: когда не в шутку занемог\n",
      "=====\n",
      "encoded: рп хдвивфю ужгб йвуфвдкн\n",
      "original: он уважать себя заставил\n",
      "decoded: он уважать себя заставил\n",
      "=====\n",
      "encoded: к нхщъж дэёховфю пж оре\n",
      "original: и лучше выдумать не мог\n",
      "decoded: и лучше выдумать не мог\n",
      "=====\n",
      "encoded: жер сткожт ётхеко пвхмв\n",
      "original: его пример другим наука\n",
      "decoded: его пример другим наука\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'encoded: {encoded[i]}\\noriginal: {data[i]}\\ndecoded: {decode(encoded[i])}\\n=====')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** как визуальная проверка, так и метрики показывают, что наша модель в случае фиксированного сдвига показывает отличные результаты (0.99 test acc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вариант №2. Смещение от 2 до 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, time: 0.093, train_loss: 2.8287146389484406, train_acc: 0.3658482142857143, test_loss: 2.1854909658432007, test_acc: 0.48839285714285713\n",
      "ep: 1, time: 0.152, train_loss: 2.085079535841942, train_acc: 0.5042410714285714, test_loss: 2.0340269804000854, test_acc: 0.48928571428571427\n",
      "ep: 2, time: 0.078, train_loss: 1.9674385040998459, train_acc: 0.5209821428571428, test_loss: 1.9513425827026367, test_acc: 0.5080357142857143\n",
      "ep: 3, time: 0.075, train_loss: 1.8817001283168793, train_acc: 0.5332589285714285, test_loss: 1.8814446926116943, test_acc: 0.5223214285714286\n",
      "ep: 4, time: 0.063, train_loss: 1.8093371540307999, train_acc: 0.5390625, test_loss: 1.8231950998306274, test_acc: 0.5223214285714286\n",
      "ep: 5, time: 0.073, train_loss: 1.7501927465200424, train_acc: 0.5439732142857143, test_loss: 1.7751359343528748, test_acc: 0.5375\n",
      "ep: 6, time: 0.083, train_loss: 1.7013835906982422, train_acc: 0.5604910714285715, test_loss: 1.7336634993553162, test_acc: 0.55\n",
      "ep: 7, time: 0.069, train_loss: 1.6589176505804062, train_acc: 0.5821428571428572, test_loss: 1.6959952116012573, test_acc: 0.5758928571428571\n",
      "ep: 8, time: 0.077, train_loss: 1.6202608793973923, train_acc: 0.5997767857142857, test_loss: 1.6608771085739136, test_acc: 0.5848214285714286\n",
      "ep: 9, time: 0.084, train_loss: 1.584336906671524, train_acc: 0.6136160714285714, test_loss: 1.6279278993606567, test_acc: 0.5982142857142857\n",
      "ep: 10, time: 0.080, train_loss: 1.550780028104782, train_acc: 0.6236607142857142, test_loss: 1.597061038017273, test_acc: 0.6098214285714286\n",
      "ep: 11, time: 0.070, train_loss: 1.5194362848997116, train_acc: 0.6361607142857143, test_loss: 1.5682043433189392, test_acc: 0.6160714285714286\n",
      "ep: 12, time: 0.084, train_loss: 1.4901515990495682, train_acc: 0.6435267857142857, test_loss: 1.5412141680717468, test_acc: 0.6160714285714286\n",
      "ep: 13, time: 0.082, train_loss: 1.4627234935760498, train_acc: 0.6497767857142858, test_loss: 1.5158875584602356, test_acc: 0.6258928571428571\n",
      "ep: 14, time: 0.075, train_loss: 1.4369222968816757, train_acc: 0.6560267857142857, test_loss: 1.49200040102005, test_acc: 0.6303571428571428\n",
      "ep: 15, time: 0.083, train_loss: 1.412522241473198, train_acc: 0.6611607142857143, test_loss: 1.4693429470062256, test_acc: 0.6366071428571428\n",
      "ep: 16, time: 0.079, train_loss: 1.3893238306045532, train_acc: 0.6654017857142858, test_loss: 1.4477355480194092, test_acc: 0.6392857142857142\n",
      "ep: 17, time: 0.082, train_loss: 1.367161601781845, train_acc: 0.6680803571428572, test_loss: 1.4270338416099548, test_acc: 0.6392857142857142\n",
      "ep: 18, time: 0.083, train_loss: 1.3459042757749557, train_acc: 0.6705357142857142, test_loss: 1.4071260690689087, test_acc: 0.6419642857142858\n",
      "ep: 19, time: 0.065, train_loss: 1.325449213385582, train_acc: 0.6729910714285714, test_loss: 1.3879263997077942, test_acc: 0.6455357142857143\n",
      "ep: 20, time: 0.065, train_loss: 1.3057169914245605, train_acc: 0.6754464285714286, test_loss: 1.3693686723709106, test_acc: 0.6464285714285715\n",
      "ep: 21, time: 0.077, train_loss: 1.2866452783346176, train_acc: 0.6779017857142857, test_loss: 1.3514009714126587, test_acc: 0.6473214285714286\n",
      "ep: 22, time: 0.073, train_loss: 1.2681840509176254, train_acc: 0.6805803571428571, test_loss: 1.3339817523956299, test_acc: 0.6526785714285714\n",
      "ep: 23, time: 0.069, train_loss: 1.2502926141023636, train_acc: 0.6850446428571428, test_loss: 1.3170764446258545, test_acc: 0.6571428571428571\n",
      "ep: 24, time: 0.074, train_loss: 1.2329368889331818, train_acc: 0.6877232142857143, test_loss: 1.300656020641327, test_acc: 0.6571428571428571\n",
      "ep: 25, time: 0.057, train_loss: 1.2160875797271729, train_acc: 0.6897321428571429, test_loss: 1.2846948504447937, test_acc: 0.65625\n",
      "ep: 26, time: 0.056, train_loss: 1.1997195333242416, train_acc: 0.6933035714285715, test_loss: 1.2691709995269775, test_acc: 0.6598214285714286\n",
      "ep: 27, time: 0.073, train_loss: 1.183810368180275, train_acc: 0.6964285714285714, test_loss: 1.2540645599365234, test_acc: 0.6660714285714285\n",
      "ep: 28, time: 0.065, train_loss: 1.1683404445648193, train_acc: 0.6991071428571428, test_loss: 1.2393577694892883, test_acc: 0.6705357142857142\n",
      "ep: 29, time: 0.066, train_loss: 1.1532920449972153, train_acc: 0.7029017857142857, test_loss: 1.2250345945358276, test_acc: 0.6741071428571429\n",
      "ep: 30, time: 0.074, train_loss: 1.1386492401361465, train_acc: 0.7055803571428572, test_loss: 1.2110809683799744, test_acc: 0.6785714285714286\n",
      "ep: 31, time: 0.069, train_loss: 1.1243975907564163, train_acc: 0.7075892857142857, test_loss: 1.1974835395812988, test_acc: 0.6803571428571429\n",
      "ep: 32, time: 0.080, train_loss: 1.1105242669582367, train_acc: 0.7102678571428571, test_loss: 1.1842304468154907, test_acc: 0.68125\n",
      "ep: 33, time: 0.069, train_loss: 1.0970172435045242, train_acc: 0.7129464285714285, test_loss: 1.1713106632232666, test_acc: 0.6866071428571429\n",
      "ep: 34, time: 0.069, train_loss: 1.0838656425476074, train_acc: 0.715625, test_loss: 1.1587145924568176, test_acc: 0.6901785714285714\n",
      "ep: 35, time: 0.074, train_loss: 1.0710593536496162, train_acc: 0.71875, test_loss: 1.1464324593544006, test_acc: 0.6910714285714286\n",
      "ep: 36, time: 0.063, train_loss: 1.0585888996720314, train_acc: 0.7205357142857143, test_loss: 1.1344561576843262, test_acc: 0.6946428571428571\n",
      "ep: 37, time: 0.067, train_loss: 1.0464456230401993, train_acc: 0.7229910714285714, test_loss: 1.1227773427963257, test_acc: 0.6955357142857143\n",
      "ep: 38, time: 0.071, train_loss: 1.0346209928393364, train_acc: 0.7254464285714286, test_loss: 1.1113886833190918, test_acc: 0.6982142857142857\n",
      "ep: 39, time: 0.062, train_loss: 1.0231071412563324, train_acc: 0.7261160714285714, test_loss: 1.1002826690673828, test_acc: 0.7\n",
      "ep: 40, time: 0.082, train_loss: 1.0118963494896889, train_acc: 0.728125, test_loss: 1.0894526839256287, test_acc: 0.7008928571428571\n",
      "ep: 41, time: 0.084, train_loss: 1.0009811520576477, train_acc: 0.7285714285714285, test_loss: 1.0788918137550354, test_acc: 0.7017857142857142\n",
      "ep: 42, time: 0.068, train_loss: 0.990354098379612, train_acc: 0.7305803571428572, test_loss: 1.068593978881836, test_acc: 0.7017857142857142\n",
      "ep: 43, time: 0.070, train_loss: 0.9800080955028534, train_acc: 0.7310267857142857, test_loss: 1.058552622795105, test_acc: 0.7026785714285714\n",
      "ep: 44, time: 0.081, train_loss: 0.9699360281229019, train_acc: 0.7314732142857143, test_loss: 1.0487615466117859, test_acc: 0.7035714285714286\n",
      "ep: 45, time: 0.069, train_loss: 0.9601306319236755, train_acc: 0.7321428571428571, test_loss: 1.0392150282859802, test_acc: 0.7044642857142858\n",
      "ep: 46, time: 0.072, train_loss: 0.9505850821733475, train_acc: 0.7337053571428571, test_loss: 1.0299067497253418, test_acc: 0.7071428571428572\n",
      "ep: 47, time: 0.073, train_loss: 0.9412922784686089, train_acc: 0.7357142857142858, test_loss: 1.0208310782909393, test_acc: 0.7089285714285715\n",
      "ep: 48, time: 0.085, train_loss: 0.9322453290224075, train_acc: 0.7370535714285714, test_loss: 1.0119822919368744, test_acc: 0.7098214285714286\n",
      "ep: 49, time: 0.073, train_loss: 0.9234370738267899, train_acc: 0.7372767857142857, test_loss: 1.0033543109893799, test_acc: 0.7133928571428572\n",
      "ep: 50, time: 0.076, train_loss: 0.9148607924580574, train_acc: 0.7388392857142857, test_loss: 0.9949418604373932, test_acc: 0.7151785714285714\n",
      "ep: 51, time: 0.082, train_loss: 0.9065096527338028, train_acc: 0.7390625, test_loss: 0.9867392480373383, test_acc: 0.7151785714285714\n",
      "ep: 52, time: 0.074, train_loss: 0.8983768597245216, train_acc: 0.7399553571428571, test_loss: 0.9787410497665405, test_acc: 0.7160714285714286\n",
      "ep: 53, time: 0.076, train_loss: 0.8904557228088379, train_acc: 0.7397321428571428, test_loss: 0.9709417819976807, test_acc: 0.7169642857142857\n",
      "ep: 54, time: 0.085, train_loss: 0.8827396631240845, train_acc: 0.740625, test_loss: 0.963336169719696, test_acc: 0.71875\n",
      "ep: 55, time: 0.094, train_loss: 0.8752221837639809, train_acc: 0.7419642857142857, test_loss: 0.9559191167354584, test_acc: 0.7196428571428571\n",
      "ep: 56, time: 0.090, train_loss: 0.8678969740867615, train_acc: 0.7421875, test_loss: 0.9486854672431946, test_acc: 0.7214285714285714\n",
      "ep: 57, time: 0.071, train_loss: 0.8607578054070473, train_acc: 0.74375, test_loss: 0.9416300654411316, test_acc: 0.7232142857142857\n",
      "ep: 58, time: 0.084, train_loss: 0.8537986055016518, train_acc: 0.7439732142857143, test_loss: 0.9347483515739441, test_acc: 0.7267857142857143\n",
      "ep: 59, time: 0.087, train_loss: 0.8470134362578392, train_acc: 0.7446428571428572, test_loss: 0.9280352294445038, test_acc: 0.7258928571428571\n",
      "ep: 60, time: 0.078, train_loss: 0.8403965905308723, train_acc: 0.7457589285714286, test_loss: 0.9214863479137421, test_acc: 0.7258928571428571\n",
      "ep: 61, time: 0.078, train_loss: 0.8339423909783363, train_acc: 0.746875, test_loss: 0.9150969088077545, test_acc: 0.7276785714285714\n",
      "ep: 62, time: 0.088, train_loss: 0.827645443379879, train_acc: 0.7479910714285715, test_loss: 0.9088624119758606, test_acc: 0.7285714285714285\n",
      "ep: 63, time: 0.086, train_loss: 0.8215004503726959, train_acc: 0.7482142857142857, test_loss: 0.902778834104538, test_acc: 0.7294642857142857\n",
      "ep: 64, time: 0.078, train_loss: 0.8155023083090782, train_acc: 0.7493303571428571, test_loss: 0.8968416154384613, test_acc: 0.7294642857142857\n",
      "ep: 65, time: 0.073, train_loss: 0.8096460402011871, train_acc: 0.7502232142857143, test_loss: 0.8910467624664307, test_acc: 0.7285714285714285\n",
      "ep: 66, time: 0.082, train_loss: 0.8039269149303436, train_acc: 0.7517857142857143, test_loss: 0.8853904008865356, test_acc: 0.7303571428571428\n",
      "ep: 67, time: 0.075, train_loss: 0.7983402609825134, train_acc: 0.7526785714285714, test_loss: 0.8798683881759644, test_acc: 0.7303571428571428\n",
      "ep: 68, time: 0.082, train_loss: 0.7928816974163055, train_acc: 0.7542410714285714, test_loss: 0.8744769096374512, test_acc: 0.7330357142857142\n",
      "ep: 69, time: 0.075, train_loss: 0.7875467613339424, train_acc: 0.7551339285714286, test_loss: 0.8692123591899872, test_acc: 0.7321428571428571\n",
      "ep: 70, time: 0.075, train_loss: 0.7823314815759659, train_acc: 0.7558035714285715, test_loss: 0.8640710413455963, test_acc: 0.7321428571428571\n",
      "ep: 71, time: 0.079, train_loss: 0.7772317677736282, train_acc: 0.7564732142857142, test_loss: 0.8590496182441711, test_acc: 0.7330357142857142\n",
      "ep: 72, time: 0.074, train_loss: 0.7722437903285027, train_acc: 0.7569196428571429, test_loss: 0.8541444838047028, test_acc: 0.7330357142857142\n",
      "ep: 73, time: 0.075, train_loss: 0.7673637568950653, train_acc: 0.7569196428571429, test_loss: 0.8493523299694061, test_acc: 0.7348214285714286\n",
      "ep: 74, time: 0.076, train_loss: 0.762588195502758, train_acc: 0.7571428571428571, test_loss: 0.8446698486804962, test_acc: 0.7357142857142858\n",
      "ep: 75, time: 0.068, train_loss: 0.7579136416316032, train_acc: 0.7582589285714286, test_loss: 0.8400940299034119, test_acc: 0.7375\n",
      "ep: 76, time: 0.076, train_loss: 0.75333671271801, train_acc: 0.7595982142857143, test_loss: 0.8356216847896576, test_acc: 0.7383928571428572\n",
      "ep: 77, time: 0.070, train_loss: 0.7488543838262558, train_acc: 0.7595982142857143, test_loss: 0.8312499821186066, test_acc: 0.7392857142857143\n",
      "ep: 78, time: 0.064, train_loss: 0.7444634288549423, train_acc: 0.7602678571428572, test_loss: 0.8269758522510529, test_acc: 0.7383928571428572\n",
      "ep: 79, time: 0.071, train_loss: 0.7401609793305397, train_acc: 0.7609375, test_loss: 0.822796642780304, test_acc: 0.7383928571428572\n",
      "ep: 80, time: 0.075, train_loss: 0.7359442114830017, train_acc: 0.7616071428571428, test_loss: 0.8187095820903778, test_acc: 0.7392857142857143\n",
      "ep: 81, time: 0.070, train_loss: 0.7318104133009911, train_acc: 0.7613839285714286, test_loss: 0.8147119879722595, test_acc: 0.7410714285714286\n",
      "ep: 82, time: 0.066, train_loss: 0.7277568653225899, train_acc: 0.7616071428571428, test_loss: 0.8108014464378357, test_acc: 0.7401785714285715\n",
      "ep: 83, time: 0.078, train_loss: 0.7237812504172325, train_acc: 0.7620535714285714, test_loss: 0.8069753050804138, test_acc: 0.7410714285714286\n",
      "ep: 84, time: 0.084, train_loss: 0.7198810428380966, train_acc: 0.7629464285714286, test_loss: 0.8032312393188477, test_acc: 0.7428571428571429\n",
      "ep: 85, time: 0.076, train_loss: 0.7160539403557777, train_acc: 0.7631696428571428, test_loss: 0.7995669543743134, test_acc: 0.74375\n",
      "ep: 86, time: 0.066, train_loss: 0.7122977077960968, train_acc: 0.7633928571428571, test_loss: 0.7959802150726318, test_acc: 0.74375\n",
      "ep: 87, time: 0.076, train_loss: 0.708610288798809, train_acc: 0.7633928571428571, test_loss: 0.7924686968326569, test_acc: 0.7464285714285714\n",
      "ep: 88, time: 0.083, train_loss: 0.7049895003437996, train_acc: 0.7642857142857142, test_loss: 0.7890304923057556, test_acc: 0.7491071428571429\n",
      "ep: 89, time: 0.074, train_loss: 0.7014334872364998, train_acc: 0.7647321428571429, test_loss: 0.785663366317749, test_acc: 0.7491071428571429\n",
      "ep: 90, time: 0.078, train_loss: 0.6979403272271156, train_acc: 0.7654017857142857, test_loss: 0.782365471124649, test_acc: 0.7491071428571429\n",
      "ep: 91, time: 0.068, train_loss: 0.6945082247257233, train_acc: 0.7662946428571429, test_loss: 0.7791348695755005, test_acc: 0.7508928571428571\n",
      "ep: 92, time: 0.091, train_loss: 0.6911353841423988, train_acc: 0.7671875, test_loss: 0.7759698033332825, test_acc: 0.7517857142857143\n",
      "ep: 93, time: 0.083, train_loss: 0.6878201588988304, train_acc: 0.76875, test_loss: 0.7728682160377502, test_acc: 0.7517857142857143\n",
      "ep: 94, time: 0.077, train_loss: 0.6845609098672867, train_acc: 0.7694196428571428, test_loss: 0.769828736782074, test_acc: 0.75\n",
      "ep: 95, time: 0.089, train_loss: 0.6813561171293259, train_acc: 0.7698660714285714, test_loss: 0.7668493688106537, test_acc: 0.75\n",
      "ep: 96, time: 0.090, train_loss: 0.678204245865345, train_acc: 0.7703125, test_loss: 0.7639287710189819, test_acc: 0.7508928571428571\n",
      "ep: 97, time: 0.072, train_loss: 0.6751038879156113, train_acc: 0.7705357142857143, test_loss: 0.7610650956630707, test_acc: 0.7517857142857143\n",
      "ep: 98, time: 0.081, train_loss: 0.6720536723732948, train_acc: 0.7705357142857143, test_loss: 0.7582570314407349, test_acc: 0.7517857142857143\n",
      "ep: 99, time: 0.076, train_loss: 0.6690522581338882, train_acc: 0.771875, test_loss: 0.7555030584335327, test_acc: 0.7526785714285714\n",
      "ep: 100, time: 0.082, train_loss: 0.6660983189940453, train_acc: 0.7725446428571429, test_loss: 0.7528018057346344, test_acc: 0.7526785714285714\n",
      "ep: 101, time: 0.075, train_loss: 0.663190670311451, train_acc: 0.7736607142857143, test_loss: 0.7501517832279205, test_acc: 0.7526785714285714\n",
      "ep: 102, time: 0.078, train_loss: 0.660328134894371, train_acc: 0.7741071428571429, test_loss: 0.7475517690181732, test_acc: 0.7526785714285714\n",
      "ep: 103, time: 0.066, train_loss: 0.6575094386935234, train_acc: 0.7754464285714285, test_loss: 0.7450003325939178, test_acc: 0.7526785714285714\n",
      "ep: 104, time: 0.071, train_loss: 0.6547335907816887, train_acc: 0.7761160714285714, test_loss: 0.7424963712692261, test_acc: 0.7526785714285714\n",
      "ep: 105, time: 0.070, train_loss: 0.6519995108246803, train_acc: 0.7765625, test_loss: 0.7400386333465576, test_acc: 0.7526785714285714\n",
      "ep: 106, time: 0.068, train_loss: 0.6493061110377312, train_acc: 0.7774553571428572, test_loss: 0.7376258075237274, test_acc: 0.7526785714285714\n",
      "ep: 107, time: 0.088, train_loss: 0.6466523855924606, train_acc: 0.7774553571428572, test_loss: 0.7352570295333862, test_acc: 0.7526785714285714\n",
      "ep: 108, time: 0.063, train_loss: 0.6440374478697777, train_acc: 0.7783482142857143, test_loss: 0.7329308092594147, test_acc: 0.7526785714285714\n",
      "ep: 109, time: 0.077, train_loss: 0.6414603069424629, train_acc: 0.7787946428571428, test_loss: 0.7306465804576874, test_acc: 0.7526785714285714\n",
      "ep: 110, time: 0.079, train_loss: 0.6389199942350388, train_acc: 0.7790178571428571, test_loss: 0.7284027934074402, test_acc: 0.7544642857142857\n",
      "ep: 111, time: 0.071, train_loss: 0.6364157050848007, train_acc: 0.7796875, test_loss: 0.7261987030506134, test_acc: 0.7535714285714286\n",
      "ep: 112, time: 0.069, train_loss: 0.6339465752243996, train_acc: 0.7805803571428571, test_loss: 0.7240333259105682, test_acc: 0.7535714285714286\n",
      "ep: 113, time: 0.060, train_loss: 0.6315117254853249, train_acc: 0.7814732142857143, test_loss: 0.7219055891036987, test_acc: 0.7535714285714286\n",
      "ep: 114, time: 0.062, train_loss: 0.6291103959083557, train_acc: 0.7819196428571429, test_loss: 0.7198145687580109, test_acc: 0.7553571428571428\n",
      "ep: 115, time: 0.074, train_loss: 0.62674181163311, train_acc: 0.7830357142857143, test_loss: 0.717759370803833, test_acc: 0.7553571428571428\n",
      "ep: 116, time: 0.064, train_loss: 0.6244052350521088, train_acc: 0.7837053571428572, test_loss: 0.7157393097877502, test_acc: 0.75625\n",
      "ep: 117, time: 0.068, train_loss: 0.6220999360084534, train_acc: 0.784375, test_loss: 0.7137532830238342, test_acc: 0.7553571428571428\n",
      "ep: 118, time: 0.086, train_loss: 0.6198252141475677, train_acc: 0.7850446428571428, test_loss: 0.7118006944656372, test_acc: 0.7553571428571428\n",
      "ep: 119, time: 0.086, train_loss: 0.617580272257328, train_acc: 0.7861607142857143, test_loss: 0.7098804712295532, test_acc: 0.7553571428571428\n",
      "ep: 120, time: 0.072, train_loss: 0.6153645291924477, train_acc: 0.7863839285714286, test_loss: 0.7079919278621674, test_acc: 0.7544642857142857\n",
      "ep: 121, time: 0.076, train_loss: 0.6131773442029953, train_acc: 0.7875, test_loss: 0.7061343789100647, test_acc: 0.7544642857142857\n",
      "ep: 122, time: 0.093, train_loss: 0.6110180616378784, train_acc: 0.7883928571428571, test_loss: 0.7043070495128632, test_acc: 0.7544642857142857\n",
      "ep: 123, time: 0.076, train_loss: 0.6088860630989075, train_acc: 0.7881696428571429, test_loss: 0.7025092244148254, test_acc: 0.7544642857142857\n",
      "ep: 124, time: 0.068, train_loss: 0.6067807599902153, train_acc: 0.7888392857142857, test_loss: 0.7007400989532471, test_acc: 0.7535714285714286\n",
      "ep: 125, time: 0.085, train_loss: 0.6047014966607094, train_acc: 0.7895089285714286, test_loss: 0.6989990472793579, test_acc: 0.7535714285714286\n",
      "ep: 126, time: 0.082, train_loss: 0.6026477292180061, train_acc: 0.7901785714285714, test_loss: 0.6972854435443878, test_acc: 0.7535714285714286\n",
      "ep: 127, time: 0.073, train_loss: 0.6006190255284309, train_acc: 0.7910714285714285, test_loss: 0.6955987513065338, test_acc: 0.7535714285714286\n",
      "ep: 128, time: 0.064, train_loss: 0.5986147001385689, train_acc: 0.7910714285714285, test_loss: 0.693938136100769, test_acc: 0.7544642857142857\n",
      "ep: 129, time: 0.081, train_loss: 0.5966343060135841, train_acc: 0.7915178571428572, test_loss: 0.6923032104969025, test_acc: 0.7544642857142857\n",
      "ep: 130, time: 0.065, train_loss: 0.5946773290634155, train_acc: 0.7919642857142857, test_loss: 0.6906929612159729, test_acc: 0.7553571428571428\n",
      "ep: 131, time: 0.066, train_loss: 0.5927432179450989, train_acc: 0.7930803571428572, test_loss: 0.6891071200370789, test_acc: 0.7553571428571428\n",
      "ep: 132, time: 0.069, train_loss: 0.5908315032720566, train_acc: 0.7930803571428572, test_loss: 0.6875451803207397, test_acc: 0.7553571428571428\n",
      "ep: 133, time: 0.070, train_loss: 0.5889417082071304, train_acc: 0.7930803571428572, test_loss: 0.6860064268112183, test_acc: 0.7544642857142857\n",
      "ep: 134, time: 0.081, train_loss: 0.5870734751224518, train_acc: 0.7939732142857143, test_loss: 0.6844905614852905, test_acc: 0.7544642857142857\n",
      "ep: 135, time: 0.085, train_loss: 0.5852263197302818, train_acc: 0.7944196428571428, test_loss: 0.6829967498779297, test_acc: 0.7544642857142857\n",
      "ep: 136, time: 0.075, train_loss: 0.5833996832370758, train_acc: 0.7948660714285715, test_loss: 0.6815248131752014, test_acc: 0.7544642857142857\n",
      "ep: 137, time: 0.066, train_loss: 0.5815932229161263, train_acc: 0.7953125, test_loss: 0.6800740659236908, test_acc: 0.7535714285714286\n",
      "ep: 138, time: 0.083, train_loss: 0.5798065587878227, train_acc: 0.7957589285714286, test_loss: 0.6786440908908844, test_acc: 0.7544642857142857\n",
      "ep: 139, time: 0.107, train_loss: 0.5780392289161682, train_acc: 0.7964285714285714, test_loss: 0.6772342622280121, test_acc: 0.7544642857142857\n",
      "ep: 140, time: 0.076, train_loss: 0.5762907862663269, train_acc: 0.7966517857142857, test_loss: 0.6758443415164948, test_acc: 0.7571428571428571\n",
      "ep: 141, time: 0.074, train_loss: 0.5745609775185585, train_acc: 0.7973214285714286, test_loss: 0.6744738221168518, test_acc: 0.7571428571428571\n",
      "ep: 142, time: 0.068, train_loss: 0.5728493854403496, train_acc: 0.7979910714285714, test_loss: 0.673122376203537, test_acc: 0.7571428571428571\n",
      "ep: 143, time: 0.079, train_loss: 0.5711555704474449, train_acc: 0.7991071428571429, test_loss: 0.6717894673347473, test_acc: 0.75625\n",
      "ep: 144, time: 0.076, train_loss: 0.5694792494177818, train_acc: 0.8, test_loss: 0.6704747676849365, test_acc: 0.7571428571428571\n",
      "ep: 145, time: 0.079, train_loss: 0.5678200051188469, train_acc: 0.8011160714285714, test_loss: 0.669177919626236, test_acc: 0.7571428571428571\n",
      "ep: 146, time: 0.063, train_loss: 0.5661775916814804, train_acc: 0.8017857142857143, test_loss: 0.6678984463214874, test_acc: 0.7580357142857143\n",
      "ep: 147, time: 0.064, train_loss: 0.5645515844225883, train_acc: 0.8020089285714286, test_loss: 0.6666360199451447, test_acc: 0.7580357142857143\n",
      "ep: 148, time: 0.076, train_loss: 0.5629417076706886, train_acc: 0.8024553571428571, test_loss: 0.6653903126716614, test_acc: 0.7571428571428571\n",
      "ep: 149, time: 0.087, train_loss: 0.5613475702702999, train_acc: 0.803125, test_loss: 0.6641608774662018, test_acc: 0.7580357142857143\n",
      "ep: 150, time: 0.087, train_loss: 0.5597689226269722, train_acc: 0.8040178571428571, test_loss: 0.6629475057125092, test_acc: 0.7580357142857143\n",
      "ep: 151, time: 0.068, train_loss: 0.5582054443657398, train_acc: 0.8046875, test_loss: 0.6617497801780701, test_acc: 0.7553571428571428\n",
      "ep: 152, time: 0.082, train_loss: 0.5566569119691849, train_acc: 0.8051339285714286, test_loss: 0.6605674624443054, test_acc: 0.75625\n",
      "ep: 153, time: 0.079, train_loss: 0.5551229082047939, train_acc: 0.8049107142857143, test_loss: 0.6594001650810242, test_acc: 0.7571428571428571\n",
      "ep: 154, time: 0.077, train_loss: 0.5536032058298588, train_acc: 0.8055803571428571, test_loss: 0.6582476794719696, test_acc: 0.7571428571428571\n",
      "ep: 155, time: 0.066, train_loss: 0.5520975366234779, train_acc: 0.8064732142857143, test_loss: 0.6571096181869507, test_acc: 0.7589285714285714\n",
      "ep: 156, time: 0.097, train_loss: 0.5506055653095245, train_acc: 0.8069196428571429, test_loss: 0.6559857130050659, test_acc: 0.7589285714285714\n",
      "ep: 157, time: 0.079, train_loss: 0.5491271056234837, train_acc: 0.8071428571428572, test_loss: 0.654875636100769, test_acc: 0.7598214285714285\n",
      "ep: 158, time: 0.076, train_loss: 0.5476618930697441, train_acc: 0.8073660714285714, test_loss: 0.6537792384624481, test_acc: 0.7598214285714285\n",
      "ep: 159, time: 0.078, train_loss: 0.5462095662951469, train_acc: 0.8078125, test_loss: 0.652696281671524, test_acc: 0.7589285714285714\n",
      "ep: 160, time: 0.109, train_loss: 0.5447699837386608, train_acc: 0.8087053571428572, test_loss: 0.6516263484954834, test_acc: 0.7589285714285714\n",
      "ep: 161, time: 0.130, train_loss: 0.5433428511023521, train_acc: 0.8087053571428572, test_loss: 0.6505691409111023, test_acc: 0.7598214285714285\n",
      "ep: 162, time: 0.081, train_loss: 0.5419279187917709, train_acc: 0.8087053571428572, test_loss: 0.6495247185230255, test_acc: 0.7607142857142857\n",
      "ep: 163, time: 0.071, train_loss: 0.540525034070015, train_acc: 0.8089285714285714, test_loss: 0.6484924256801605, test_acc: 0.7616071428571428\n",
      "ep: 164, time: 0.079, train_loss: 0.5391338355839252, train_acc: 0.809375, test_loss: 0.6474723815917969, test_acc: 0.7625\n",
      "ep: 165, time: 0.069, train_loss: 0.5377541929483414, train_acc: 0.8095982142857143, test_loss: 0.6464642584323883, test_acc: 0.7625\n",
      "ep: 166, time: 0.089, train_loss: 0.536385852843523, train_acc: 0.8107142857142857, test_loss: 0.6454677879810333, test_acc: 0.7607142857142857\n",
      "ep: 167, time: 0.070, train_loss: 0.5350285805761814, train_acc: 0.8111607142857142, test_loss: 0.6444827914237976, test_acc: 0.7616071428571428\n",
      "ep: 168, time: 0.065, train_loss: 0.5336822085082531, train_acc: 0.8116071428571429, test_loss: 0.6435090899467468, test_acc: 0.7616071428571428\n",
      "ep: 169, time: 0.071, train_loss: 0.5323464572429657, train_acc: 0.8116071428571429, test_loss: 0.6425462663173676, test_acc: 0.7616071428571428\n",
      "ep: 170, time: 0.074, train_loss: 0.5310211926698685, train_acc: 0.8125, test_loss: 0.641594409942627, test_acc: 0.7633928571428571\n",
      "ep: 171, time: 0.075, train_loss: 0.5297061465680599, train_acc: 0.8127232142857143, test_loss: 0.640653133392334, test_acc: 0.7616071428571428\n",
      "ep: 172, time: 0.067, train_loss: 0.5284011773765087, train_acc: 0.8129464285714286, test_loss: 0.6397223472595215, test_acc: 0.7616071428571428\n",
      "ep: 173, time: 0.069, train_loss: 0.5271060839295387, train_acc: 0.8138392857142858, test_loss: 0.6388016939163208, test_acc: 0.7616071428571428\n",
      "ep: 174, time: 0.083, train_loss: 0.5258206427097321, train_acc: 0.8140625, test_loss: 0.6378911137580872, test_acc: 0.7625\n",
      "ep: 175, time: 0.082, train_loss: 0.5245446376502514, train_acc: 0.8142857142857143, test_loss: 0.6369905173778534, test_acc: 0.7625\n",
      "ep: 176, time: 0.062, train_loss: 0.5232779607176781, train_acc: 0.8142857142857143, test_loss: 0.6360994875431061, test_acc: 0.7625\n",
      "ep: 177, time: 0.078, train_loss: 0.5220203511416912, train_acc: 0.8142857142857143, test_loss: 0.6352181434631348, test_acc: 0.7625\n",
      "ep: 178, time: 0.083, train_loss: 0.5207717753946781, train_acc: 0.8154017857142857, test_loss: 0.6343460977077484, test_acc: 0.7625\n",
      "ep: 179, time: 0.060, train_loss: 0.5195319280028343, train_acc: 0.815625, test_loss: 0.6334832906723022, test_acc: 0.7642857142857142\n",
      "ep: 180, time: 0.060, train_loss: 0.518300723284483, train_acc: 0.8158482142857143, test_loss: 0.6326295435428619, test_acc: 0.7651785714285714\n",
      "ep: 181, time: 0.071, train_loss: 0.5170778967440128, train_acc: 0.8162946428571428, test_loss: 0.6317844688892365, test_acc: 0.7651785714285714\n",
      "ep: 182, time: 0.074, train_loss: 0.5158634036779404, train_acc: 0.8171875, test_loss: 0.6309483349323273, test_acc: 0.7651785714285714\n",
      "ep: 183, time: 0.077, train_loss: 0.5146569795906544, train_acc: 0.8174107142857143, test_loss: 0.6301206648349762, test_acc: 0.7651785714285714\n",
      "ep: 184, time: 0.077, train_loss: 0.5134584903717041, train_acc: 0.8178571428571428, test_loss: 0.6293014883995056, test_acc: 0.7651785714285714\n",
      "ep: 185, time: 0.083, train_loss: 0.5122678987681866, train_acc: 0.8183035714285715, test_loss: 0.6284905970096588, test_acc: 0.7651785714285714\n",
      "ep: 186, time: 0.084, train_loss: 0.5110848359763622, train_acc: 0.8194196428571429, test_loss: 0.6276878118515015, test_acc: 0.7651785714285714\n",
      "ep: 187, time: 0.068, train_loss: 0.5099093653261662, train_acc: 0.8196428571428571, test_loss: 0.6268930733203888, test_acc: 0.7651785714285714\n",
      "ep: 188, time: 0.076, train_loss: 0.5087412036955357, train_acc: 0.8200892857142857, test_loss: 0.6261060833930969, test_acc: 0.7651785714285714\n",
      "ep: 189, time: 0.062, train_loss: 0.5075802765786648, train_acc: 0.8209821428571429, test_loss: 0.6253269016742706, test_acc: 0.7642857142857142\n",
      "ep: 190, time: 0.071, train_loss: 0.5064264014363289, train_acc: 0.8214285714285714, test_loss: 0.6245553195476532, test_acc: 0.7642857142857142\n",
      "ep: 191, time: 0.060, train_loss: 0.5052794925868511, train_acc: 0.821875, test_loss: 0.6237911880016327, test_acc: 0.7642857142857142\n",
      "ep: 192, time: 0.075, train_loss: 0.5041393414139748, train_acc: 0.8220982142857143, test_loss: 0.6230344772338867, test_acc: 0.7642857142857142\n",
      "ep: 193, time: 0.070, train_loss: 0.503005888313055, train_acc: 0.8220982142857143, test_loss: 0.6222849190235138, test_acc: 0.7642857142857142\n",
      "ep: 194, time: 0.075, train_loss: 0.5018790140748024, train_acc: 0.8216517857142858, test_loss: 0.6215425133705139, test_acc: 0.7642857142857142\n",
      "ep: 195, time: 0.065, train_loss: 0.5007585361599922, train_acc: 0.8223214285714285, test_loss: 0.6208069622516632, test_acc: 0.7642857142857142\n",
      "ep: 196, time: 0.064, train_loss: 0.49964430555701256, train_acc: 0.8225446428571429, test_loss: 0.620078444480896, test_acc: 0.7642857142857142\n",
      "ep: 197, time: 0.071, train_loss: 0.49853628873825073, train_acc: 0.8229910714285714, test_loss: 0.619356632232666, test_acc: 0.7642857142857142\n",
      "ep: 198, time: 0.075, train_loss: 0.4974343031644821, train_acc: 0.8229910714285714, test_loss: 0.6186413764953613, test_acc: 0.7642857142857142\n",
      "ep: 199, time: 0.059, train_loss: 0.49633824452757835, train_acc: 0.8245535714285714, test_loss: 0.6179327964782715, test_acc: 0.7660714285714286\n",
      "ep: 200, time: 0.062, train_loss: 0.4952480308711529, train_acc: 0.8258928571428571, test_loss: 0.6172306537628174, test_acc: 0.7660714285714286\n",
      "ep: 201, time: 0.069, train_loss: 0.49416350945830345, train_acc: 0.8265625, test_loss: 0.6165347993373871, test_acc: 0.7642857142857142\n",
      "ep: 202, time: 0.068, train_loss: 0.4930845946073532, train_acc: 0.8270089285714286, test_loss: 0.615845263004303, test_acc: 0.7651785714285714\n",
      "ep: 203, time: 0.069, train_loss: 0.4920111931860447, train_acc: 0.8270089285714286, test_loss: 0.6151617467403412, test_acc: 0.7660714285714286\n",
      "ep: 204, time: 0.078, train_loss: 0.4909431152045727, train_acc: 0.8272321428571429, test_loss: 0.6144842505455017, test_acc: 0.7660714285714286\n",
      "ep: 205, time: 0.079, train_loss: 0.48988039791584015, train_acc: 0.8270089285714286, test_loss: 0.6138128340244293, test_acc: 0.7678571428571429\n",
      "ep: 206, time: 0.081, train_loss: 0.4888228140771389, train_acc: 0.8276785714285714, test_loss: 0.6131471693515778, test_acc: 0.7678571428571429\n",
      "ep: 207, time: 0.062, train_loss: 0.48777036368846893, train_acc: 0.8274553571428571, test_loss: 0.6124873459339142, test_acc: 0.7669642857142858\n",
      "ep: 208, time: 0.077, train_loss: 0.4867228530347347, train_acc: 0.8274553571428571, test_loss: 0.6118330955505371, test_acc: 0.7660714285714286\n",
      "ep: 209, time: 0.065, train_loss: 0.485680241137743, train_acc: 0.8279017857142857, test_loss: 0.6111845672130585, test_acc: 0.7660714285714286\n",
      "ep: 210, time: 0.057, train_loss: 0.4846424497663975, train_acc: 0.8279017857142857, test_loss: 0.6105414628982544, test_acc: 0.7660714285714286\n",
      "ep: 211, time: 0.069, train_loss: 0.4836093336343765, train_acc: 0.8285714285714286, test_loss: 0.6099037230014801, test_acc: 0.7660714285714286\n",
      "ep: 212, time: 0.075, train_loss: 0.48258087411522865, train_acc: 0.8292410714285714, test_loss: 0.6092712879180908, test_acc: 0.7660714285714286\n",
      "ep: 213, time: 0.068, train_loss: 0.4815569221973419, train_acc: 0.8292410714285714, test_loss: 0.6086442768573761, test_acc: 0.7660714285714286\n",
      "ep: 214, time: 0.058, train_loss: 0.48053740337491035, train_acc: 0.8294642857142858, test_loss: 0.6080223619937897, test_acc: 0.7660714285714286\n",
      "ep: 215, time: 0.080, train_loss: 0.47952229157090187, train_acc: 0.8296875, test_loss: 0.6074056327342987, test_acc: 0.7660714285714286\n",
      "ep: 216, time: 0.069, train_loss: 0.47851141542196274, train_acc: 0.8299107142857143, test_loss: 0.6067938208580017, test_acc: 0.7669642857142858\n",
      "ep: 217, time: 0.060, train_loss: 0.47750475257635117, train_acc: 0.8303571428571429, test_loss: 0.6061869859695435, test_acc: 0.7678571428571429\n",
      "ep: 218, time: 0.067, train_loss: 0.4765022359788418, train_acc: 0.8305803571428572, test_loss: 0.6055850088596344, test_acc: 0.7678571428571429\n",
      "ep: 219, time: 0.070, train_loss: 0.47550368681550026, train_acc: 0.83125, test_loss: 0.6049879193305969, test_acc: 0.7678571428571429\n",
      "ep: 220, time: 0.072, train_loss: 0.47450917959213257, train_acc: 0.83125, test_loss: 0.6043955683708191, test_acc: 0.7678571428571429\n",
      "ep: 221, time: 0.058, train_loss: 0.4735185168683529, train_acc: 0.8314732142857143, test_loss: 0.6038078665733337, test_acc: 0.76875\n",
      "ep: 222, time: 0.079, train_loss: 0.4725317172706127, train_acc: 0.8323660714285714, test_loss: 0.6032248735427856, test_acc: 0.7705357142857143\n",
      "ep: 223, time: 0.078, train_loss: 0.4715486541390419, train_acc: 0.8325892857142857, test_loss: 0.6026464104652405, test_acc: 0.7705357142857143\n",
      "ep: 224, time: 0.072, train_loss: 0.4705692380666733, train_acc: 0.8328125, test_loss: 0.6020722389221191, test_acc: 0.7705357142857143\n",
      "ep: 225, time: 0.078, train_loss: 0.46959347277879715, train_acc: 0.8330357142857143, test_loss: 0.6015026271343231, test_acc: 0.7705357142857143\n",
      "ep: 226, time: 0.066, train_loss: 0.46862121671438217, train_acc: 0.8334821428571428, test_loss: 0.6009373962879181, test_acc: 0.7705357142857143\n",
      "ep: 227, time: 0.066, train_loss: 0.46765243634581566, train_acc: 0.8337053571428571, test_loss: 0.6003764569759369, test_acc: 0.7705357142857143\n",
      "ep: 228, time: 0.072, train_loss: 0.4666871055960655, train_acc: 0.8339285714285715, test_loss: 0.5998198091983795, test_acc: 0.7705357142857143\n",
      "ep: 229, time: 0.073, train_loss: 0.465725127607584, train_acc: 0.8339285714285715, test_loss: 0.5992674231529236, test_acc: 0.7705357142857143\n",
      "ep: 230, time: 0.074, train_loss: 0.4647664278745651, train_acc: 0.834375, test_loss: 0.5987190008163452, test_acc: 0.7705357142857143\n",
      "ep: 231, time: 0.070, train_loss: 0.463810957968235, train_acc: 0.834375, test_loss: 0.5981748104095459, test_acc: 0.7714285714285715\n",
      "ep: 232, time: 0.067, train_loss: 0.4628586247563362, train_acc: 0.8352678571428571, test_loss: 0.5976345539093018, test_acc: 0.7714285714285715\n",
      "ep: 233, time: 0.062, train_loss: 0.4619094468653202, train_acc: 0.8354910714285714, test_loss: 0.59709832072258, test_acc: 0.7714285714285715\n",
      "ep: 234, time: 0.074, train_loss: 0.46096333861351013, train_acc: 0.8357142857142857, test_loss: 0.5965660214424133, test_acc: 0.7714285714285715\n",
      "ep: 235, time: 0.063, train_loss: 0.46002018451690674, train_acc: 0.8357142857142857, test_loss: 0.5960376262664795, test_acc: 0.7705357142857143\n",
      "ep: 236, time: 0.064, train_loss: 0.45907997339963913, train_acc: 0.8361607142857143, test_loss: 0.5955130159854889, test_acc: 0.7705357142857143\n",
      "ep: 237, time: 0.060, train_loss: 0.4581426903605461, train_acc: 0.8366071428571429, test_loss: 0.5949922502040863, test_acc: 0.7705357142857143\n",
      "ep: 238, time: 0.057, train_loss: 0.45720821991562843, train_acc: 0.8366071428571429, test_loss: 0.5944751799106598, test_acc: 0.7714285714285715\n",
      "ep: 239, time: 0.078, train_loss: 0.4562765099108219, train_acc: 0.8368303571428571, test_loss: 0.5939617455005646, test_acc: 0.7732142857142857\n",
      "ep: 240, time: 0.072, train_loss: 0.4553475193679333, train_acc: 0.8368303571428571, test_loss: 0.5934520959854126, test_acc: 0.7723214285714286\n",
      "ep: 241, time: 0.078, train_loss: 0.4544212855398655, train_acc: 0.8368303571428571, test_loss: 0.5929460227489471, test_acc: 0.7723214285714286\n",
      "ep: 242, time: 0.061, train_loss: 0.453497689217329, train_acc: 0.8370535714285714, test_loss: 0.5924434661865234, test_acc: 0.7723214285714286\n",
      "ep: 243, time: 0.071, train_loss: 0.4525766372680664, train_acc: 0.8375, test_loss: 0.591944545507431, test_acc: 0.7723214285714286\n",
      "ep: 244, time: 0.072, train_loss: 0.45165810734033585, train_acc: 0.8388392857142857, test_loss: 0.5914490520954132, test_acc: 0.7723214285714286\n",
      "ep: 245, time: 0.088, train_loss: 0.4507421404123306, train_acc: 0.8390625, test_loss: 0.5909569263458252, test_acc: 0.7723214285714286\n",
      "ep: 246, time: 0.084, train_loss: 0.4498285874724388, train_acc: 0.8388392857142857, test_loss: 0.5904682874679565, test_acc: 0.7723214285714286\n",
      "ep: 247, time: 0.069, train_loss: 0.4489174634218216, train_acc: 0.8392857142857143, test_loss: 0.5899830162525177, test_acc: 0.7723214285714286\n",
      "ep: 248, time: 0.076, train_loss: 0.4480086676776409, train_acc: 0.8399553571428572, test_loss: 0.5895010530948639, test_acc: 0.7723214285714286\n",
      "ep: 249, time: 0.073, train_loss: 0.44710221514105797, train_acc: 0.8401785714285714, test_loss: 0.5890224874019623, test_acc: 0.7732142857142857\n",
      "ep: 250, time: 0.074, train_loss: 0.4461980275809765, train_acc: 0.8410714285714286, test_loss: 0.588547021150589, test_acc: 0.7732142857142857\n",
      "ep: 251, time: 0.059, train_loss: 0.4452960900962353, train_acc: 0.8415178571428571, test_loss: 0.5880750119686127, test_acc: 0.7732142857142857\n",
      "ep: 252, time: 0.077, train_loss: 0.44439633563160896, train_acc: 0.8419642857142857, test_loss: 0.5876060128211975, test_acc: 0.7732142857142857\n",
      "ep: 253, time: 0.068, train_loss: 0.44349876046180725, train_acc: 0.8428571428571429, test_loss: 0.5871402025222778, test_acc: 0.7732142857142857\n",
      "ep: 254, time: 0.061, train_loss: 0.44260323792696, train_acc: 0.84375, test_loss: 0.5866774618625641, test_acc: 0.7732142857142857\n",
      "ep: 255, time: 0.082, train_loss: 0.44170983508229256, train_acc: 0.8441964285714286, test_loss: 0.5862179100513458, test_acc: 0.7732142857142857\n",
      "ep: 256, time: 0.069, train_loss: 0.4408184885978699, train_acc: 0.8446428571428571, test_loss: 0.5857614576816559, test_acc: 0.7732142857142857\n",
      "ep: 257, time: 0.077, train_loss: 0.4399290941655636, train_acc: 0.8448660714285714, test_loss: 0.5853080451488495, test_acc: 0.7741071428571429\n",
      "ep: 258, time: 0.070, train_loss: 0.4390416815876961, train_acc: 0.8455357142857143, test_loss: 0.5848574936389923, test_acc: 0.7741071428571429\n",
      "ep: 259, time: 0.078, train_loss: 0.4381561912596226, train_acc: 0.8459821428571429, test_loss: 0.5844100713729858, test_acc: 0.775\n",
      "ep: 260, time: 0.077, train_loss: 0.43727267906069756, train_acc: 0.8459821428571429, test_loss: 0.5839655995368958, test_acc: 0.775\n",
      "ep: 261, time: 0.070, train_loss: 0.4363909624516964, train_acc: 0.8462053571428572, test_loss: 0.5835239291191101, test_acc: 0.7758928571428572\n",
      "ep: 262, time: 0.065, train_loss: 0.4355110451579094, train_acc: 0.8466517857142857, test_loss: 0.5830852389335632, test_acc: 0.7767857142857143\n",
      "ep: 263, time: 0.075, train_loss: 0.43463294208049774, train_acc: 0.8470982142857143, test_loss: 0.582649439573288, test_acc: 0.7767857142857143\n",
      "ep: 264, time: 0.075, train_loss: 0.43375658243894577, train_acc: 0.8473214285714286, test_loss: 0.5822164118289948, test_acc: 0.7767857142857143\n",
      "ep: 265, time: 0.065, train_loss: 0.43288198858499527, train_acc: 0.8475446428571428, test_loss: 0.5817863047122955, test_acc: 0.7767857142857143\n",
      "ep: 266, time: 0.069, train_loss: 0.4320090673863888, train_acc: 0.8475446428571428, test_loss: 0.581358939409256, test_acc: 0.7767857142857143\n",
      "ep: 267, time: 0.061, train_loss: 0.431137777864933, train_acc: 0.8479910714285714, test_loss: 0.5809342861175537, test_acc: 0.7767857142857143\n",
      "ep: 268, time: 0.061, train_loss: 0.43026816099882126, train_acc: 0.8482142857142857, test_loss: 0.5805124640464783, test_acc: 0.7767857142857143\n",
      "ep: 269, time: 0.076, train_loss: 0.42940009757876396, train_acc: 0.8484375, test_loss: 0.5800932943820953, test_acc: 0.7776785714285714\n",
      "ep: 270, time: 0.073, train_loss: 0.4285336472094059, train_acc: 0.8486607142857143, test_loss: 0.5796768963336945, test_acc: 0.7785714285714286\n",
      "ep: 271, time: 0.074, train_loss: 0.4276687428355217, train_acc: 0.8491071428571428, test_loss: 0.5792631208896637, test_acc: 0.7785714285714286\n",
      "ep: 272, time: 0.069, train_loss: 0.4268052577972412, train_acc: 0.8491071428571428, test_loss: 0.5788519978523254, test_acc: 0.7794642857142857\n",
      "ep: 273, time: 0.076, train_loss: 0.42594336345791817, train_acc: 0.8491071428571428, test_loss: 0.5784436166286469, test_acc: 0.7794642857142857\n",
      "ep: 274, time: 0.078, train_loss: 0.42508287727832794, train_acc: 0.8495535714285715, test_loss: 0.5780377089977264, test_acc: 0.7794642857142857\n",
      "ep: 275, time: 0.076, train_loss: 0.42422381043434143, train_acc: 0.8497767857142857, test_loss: 0.5776344537734985, test_acc: 0.7794642857142857\n",
      "ep: 276, time: 0.073, train_loss: 0.42336616665124893, train_acc: 0.8497767857142857, test_loss: 0.5772338211536407, test_acc: 0.7803571428571429\n",
      "ep: 277, time: 0.074, train_loss: 0.4225098378956318, train_acc: 0.8506696428571429, test_loss: 0.5768356323242188, test_acc: 0.7803571428571429\n",
      "ep: 278, time: 0.074, train_loss: 0.42165492475032806, train_acc: 0.8508928571428571, test_loss: 0.5764401257038116, test_acc: 0.7803571428571429\n",
      "ep: 279, time: 0.072, train_loss: 0.4208012856543064, train_acc: 0.8511160714285714, test_loss: 0.5760470330715179, test_acc: 0.7803571428571429\n",
      "ep: 280, time: 0.061, train_loss: 0.4199489653110504, train_acc: 0.8513392857142857, test_loss: 0.5756564736366272, test_acc: 0.7803571428571429\n",
      "ep: 281, time: 0.065, train_loss: 0.4190978929400444, train_acc: 0.8517857142857143, test_loss: 0.5752683877944946, test_acc: 0.7803571428571429\n",
      "ep: 282, time: 0.060, train_loss: 0.4182480536401272, train_acc: 0.8522321428571429, test_loss: 0.5748827755451202, test_acc: 0.78125\n",
      "ep: 283, time: 0.067, train_loss: 0.41739945113658905, train_acc: 0.8522321428571429, test_loss: 0.5744996964931488, test_acc: 0.78125\n",
      "ep: 284, time: 0.082, train_loss: 0.416552048176527, train_acc: 0.8526785714285714, test_loss: 0.574118971824646, test_acc: 0.78125\n",
      "ep: 285, time: 0.072, train_loss: 0.4157058112323284, train_acc: 0.8529017857142858, test_loss: 0.573740690946579, test_acc: 0.7821428571428571\n",
      "ep: 286, time: 0.072, train_loss: 0.41486071795225143, train_acc: 0.8526785714285714, test_loss: 0.573364794254303, test_acc: 0.7821428571428571\n",
      "ep: 287, time: 0.069, train_loss: 0.4140167236328125, train_acc: 0.8535714285714285, test_loss: 0.5729912221431732, test_acc: 0.7830357142857143\n",
      "ep: 288, time: 0.062, train_loss: 0.4131738319993019, train_acc: 0.8535714285714285, test_loss: 0.5726201236248016, test_acc: 0.7839285714285714\n",
      "ep: 289, time: 0.084, train_loss: 0.4123319946229458, train_acc: 0.8540178571428572, test_loss: 0.5722513496875763, test_acc: 0.7830357142857143\n",
      "ep: 290, time: 0.067, train_loss: 0.4114912860095501, train_acc: 0.8544642857142857, test_loss: 0.5718849897384644, test_acc: 0.7830357142857143\n",
      "ep: 291, time: 0.064, train_loss: 0.4106515608727932, train_acc: 0.8546875, test_loss: 0.5715208351612091, test_acc: 0.7830357142857143\n",
      "ep: 292, time: 0.093, train_loss: 0.40981288999319077, train_acc: 0.8551339285714286, test_loss: 0.571159154176712, test_acc: 0.7830357142857143\n",
      "ep: 293, time: 0.080, train_loss: 0.40897515788674355, train_acc: 0.8553571428571428, test_loss: 0.570799708366394, test_acc: 0.7830357142857143\n",
      "ep: 294, time: 0.076, train_loss: 0.4081384129822254, train_acc: 0.8555803571428572, test_loss: 0.5704424977302551, test_acc: 0.7839285714285714\n",
      "ep: 295, time: 0.082, train_loss: 0.4073025956749916, train_acc: 0.85625, test_loss: 0.5700876414775848, test_acc: 0.7839285714285714\n",
      "ep: 296, time: 0.074, train_loss: 0.406467754393816, train_acc: 0.8569196428571428, test_loss: 0.569735050201416, test_acc: 0.7839285714285714\n",
      "ep: 297, time: 0.084, train_loss: 0.4056338369846344, train_acc: 0.8575892857142857, test_loss: 0.569384753704071, test_acc: 0.7830357142857143\n",
      "ep: 298, time: 0.084, train_loss: 0.40480073913931847, train_acc: 0.8582589285714286, test_loss: 0.5690366625785828, test_acc: 0.7839285714285714\n",
      "ep: 299, time: 0.074, train_loss: 0.40396858751773834, train_acc: 0.8589285714285714, test_loss: 0.568690836429596, test_acc: 0.7839285714285714\n",
      "ep: 300, time: 0.070, train_loss: 0.4031372107565403, train_acc: 0.859375, test_loss: 0.568347156047821, test_acc: 0.7848214285714286\n",
      "ep: 301, time: 0.078, train_loss: 0.4023067019879818, train_acc: 0.8595982142857143, test_loss: 0.56800577044487, test_acc: 0.7848214285714286\n",
      "ep: 302, time: 0.077, train_loss: 0.40147700533270836, train_acc: 0.8595982142857143, test_loss: 0.5676665604114532, test_acc: 0.7848214285714286\n",
      "ep: 303, time: 0.072, train_loss: 0.4006481282413006, train_acc: 0.8598214285714286, test_loss: 0.5673295557498932, test_acc: 0.7848214285714286\n",
      "ep: 304, time: 0.082, train_loss: 0.3998200446367264, train_acc: 0.8602678571428571, test_loss: 0.566994845867157, test_acc: 0.7848214285714286\n",
      "ep: 305, time: 0.073, train_loss: 0.3989926762878895, train_acc: 0.8609375, test_loss: 0.5666621923446655, test_acc: 0.7866071428571428\n",
      "ep: 306, time: 0.075, train_loss: 0.39816607907414436, train_acc: 0.8618303571428572, test_loss: 0.5663316547870636, test_acc: 0.7866071428571428\n",
      "ep: 307, time: 0.078, train_loss: 0.39734021201729774, train_acc: 0.8620535714285714, test_loss: 0.5660034716129303, test_acc: 0.7875\n",
      "ep: 308, time: 0.074, train_loss: 0.39651504531502724, train_acc: 0.8622767857142857, test_loss: 0.565677285194397, test_acc: 0.7875\n",
      "ep: 309, time: 0.083, train_loss: 0.39569054543972015, train_acc: 0.8625, test_loss: 0.5653533637523651, test_acc: 0.7875\n",
      "ep: 310, time: 0.080, train_loss: 0.39486677944660187, train_acc: 0.8625, test_loss: 0.565031498670578, test_acc: 0.7875\n",
      "ep: 311, time: 0.073, train_loss: 0.3940436989068985, train_acc: 0.8625, test_loss: 0.5647118389606476, test_acc: 0.7875\n",
      "ep: 312, time: 0.070, train_loss: 0.3932212181389332, train_acc: 0.8629464285714286, test_loss: 0.5643942356109619, test_acc: 0.7875\n",
      "ep: 313, time: 0.078, train_loss: 0.3923994041979313, train_acc: 0.8633928571428572, test_loss: 0.5640788674354553, test_acc: 0.7883928571428571\n",
      "ep: 314, time: 0.079, train_loss: 0.39157819747924805, train_acc: 0.8633928571428572, test_loss: 0.5637655258178711, test_acc: 0.7892857142857143\n",
      "ep: 315, time: 0.069, train_loss: 0.39075757190585136, train_acc: 0.8638392857142857, test_loss: 0.5634543001651764, test_acc: 0.7901785714285714\n",
      "ep: 316, time: 0.073, train_loss: 0.3899375647306442, train_acc: 0.8640625, test_loss: 0.5631451308727264, test_acc: 0.7901785714285714\n",
      "ep: 317, time: 0.079, train_loss: 0.38911813125014305, train_acc: 0.8645089285714286, test_loss: 0.5628381967544556, test_acc: 0.7892857142857143\n",
      "ep: 318, time: 0.080, train_loss: 0.38829928264021873, train_acc: 0.8649553571428571, test_loss: 0.5625331997871399, test_acc: 0.7892857142857143\n",
      "ep: 319, time: 0.087, train_loss: 0.3874809816479683, train_acc: 0.8651785714285715, test_loss: 0.5622303187847137, test_acc: 0.7892857142857143\n",
      "ep: 320, time: 0.078, train_loss: 0.3866632282733917, train_acc: 0.8658482142857142, test_loss: 0.5619296729564667, test_acc: 0.7892857142857143\n",
      "ep: 321, time: 0.076, train_loss: 0.38584599643945694, train_acc: 0.8660714285714286, test_loss: 0.5616310238838196, test_acc: 0.7901785714285714\n",
      "ep: 322, time: 0.088, train_loss: 0.38502928242087364, train_acc: 0.8660714285714286, test_loss: 0.5613344013690948, test_acc: 0.7901785714285714\n",
      "ep: 323, time: 0.078, train_loss: 0.38421300426125526, train_acc: 0.8669642857142857, test_loss: 0.561039924621582, test_acc: 0.7901785714285714\n",
      "ep: 324, time: 0.072, train_loss: 0.38339727371931076, train_acc: 0.8676339285714286, test_loss: 0.5607473850250244, test_acc: 0.7901785714285714\n",
      "ep: 325, time: 0.088, train_loss: 0.38258203864097595, train_acc: 0.8678571428571429, test_loss: 0.5604570209980011, test_acc: 0.7901785714285714\n",
      "ep: 326, time: 0.085, train_loss: 0.38176725059747696, train_acc: 0.8683035714285714, test_loss: 0.5601687133312225, test_acc: 0.7901785714285714\n",
      "ep: 327, time: 0.086, train_loss: 0.38095293566584587, train_acc: 0.86875, test_loss: 0.5598824918270111, test_acc: 0.7901785714285714\n",
      "ep: 328, time: 0.060, train_loss: 0.3801390454173088, train_acc: 0.8696428571428572, test_loss: 0.5595982372760773, test_acc: 0.7910714285714285\n",
      "ep: 329, time: 0.067, train_loss: 0.37932562083005905, train_acc: 0.8698660714285714, test_loss: 0.5593160688877106, test_acc: 0.7919642857142857\n",
      "ep: 330, time: 0.081, train_loss: 0.3785126619040966, train_acc: 0.8700892857142857, test_loss: 0.5590360462665558, test_acc: 0.7928571428571428\n",
      "ep: 331, time: 0.076, train_loss: 0.3777000457048416, train_acc: 0.8703125, test_loss: 0.5587579309940338, test_acc: 0.7928571428571428\n",
      "ep: 332, time: 0.075, train_loss: 0.37688785418868065, train_acc: 0.8707589285714286, test_loss: 0.5584819614887238, test_acc: 0.7928571428571428\n",
      "ep: 333, time: 0.085, train_loss: 0.3760760761797428, train_acc: 0.8712053571428572, test_loss: 0.5582078993320465, test_acc: 0.7928571428571428\n",
      "ep: 334, time: 0.078, train_loss: 0.3752647005021572, train_acc: 0.8720982142857143, test_loss: 0.557936042547226, test_acc: 0.7928571428571428\n",
      "ep: 335, time: 0.081, train_loss: 0.37445366010069847, train_acc: 0.8729910714285715, test_loss: 0.5576660335063934, test_acc: 0.7928571428571428\n",
      "ep: 336, time: 0.074, train_loss: 0.37364301458001137, train_acc: 0.8729910714285715, test_loss: 0.5573982894420624, test_acc: 0.7919642857142857\n",
      "ep: 337, time: 0.076, train_loss: 0.3728327825665474, train_acc: 0.8732142857142857, test_loss: 0.5571325421333313, test_acc: 0.7919642857142857\n",
      "ep: 338, time: 0.076, train_loss: 0.3720228672027588, train_acc: 0.8736607142857142, test_loss: 0.5568687915802002, test_acc: 0.7919642857142857\n",
      "ep: 339, time: 0.062, train_loss: 0.3712133429944515, train_acc: 0.8741071428571429, test_loss: 0.5566070377826691, test_acc: 0.7919642857142857\n",
      "ep: 340, time: 0.068, train_loss: 0.37040411308407784, train_acc: 0.8756696428571429, test_loss: 0.5563473701477051, test_acc: 0.79375\n",
      "ep: 341, time: 0.079, train_loss: 0.3695952482521534, train_acc: 0.8763392857142858, test_loss: 0.5560897588729858, test_acc: 0.7946428571428571\n",
      "ep: 342, time: 0.066, train_loss: 0.36878669634461403, train_acc: 0.8767857142857143, test_loss: 0.5558341443538666, test_acc: 0.7946428571428571\n",
      "ep: 343, time: 0.056, train_loss: 0.36797843873500824, train_acc: 0.8776785714285714, test_loss: 0.5555805563926697, test_acc: 0.7946428571428571\n",
      "ep: 344, time: 0.073, train_loss: 0.3671705536544323, train_acc: 0.8779017857142857, test_loss: 0.5553290843963623, test_acc: 0.7955357142857142\n",
      "ep: 345, time: 0.065, train_loss: 0.36636294424533844, train_acc: 0.8783482142857143, test_loss: 0.5550795197486877, test_acc: 0.7964285714285714\n",
      "ep: 346, time: 0.063, train_loss: 0.36555565148591995, train_acc: 0.8785714285714286, test_loss: 0.5548320412635803, test_acc: 0.7964285714285714\n",
      "ep: 347, time: 0.060, train_loss: 0.36474864929914474, train_acc: 0.8787946428571428, test_loss: 0.5545866191387177, test_acc: 0.7964285714285714\n",
      "ep: 348, time: 0.064, train_loss: 0.3639419488608837, train_acc: 0.8792410714285714, test_loss: 0.5543432533740997, test_acc: 0.7964285714285714\n",
      "ep: 349, time: 0.071, train_loss: 0.36313556879758835, train_acc: 0.8794642857142857, test_loss: 0.554101973772049, test_acc: 0.7964285714285714\n",
      "ep: 350, time: 0.077, train_loss: 0.3623294197022915, train_acc: 0.8808035714285715, test_loss: 0.5538626909255981, test_acc: 0.7964285714285714\n",
      "ep: 351, time: 0.079, train_loss: 0.3615235835313797, train_acc: 0.8808035714285715, test_loss: 0.5536254346370697, test_acc: 0.7973214285714286\n",
      "ep: 352, time: 0.072, train_loss: 0.3607180416584015, train_acc: 0.8814732142857142, test_loss: 0.5533902645111084, test_acc: 0.7982142857142858\n",
      "ep: 353, time: 0.074, train_loss: 0.359912745654583, train_acc: 0.8823660714285714, test_loss: 0.5531570911407471, test_acc: 0.7982142857142858\n",
      "ep: 354, time: 0.073, train_loss: 0.35910770669579506, train_acc: 0.8832589285714286, test_loss: 0.5529260337352753, test_acc: 0.7982142857142858\n",
      "ep: 355, time: 0.070, train_loss: 0.3583029769361019, train_acc: 0.8832589285714286, test_loss: 0.5526968836784363, test_acc: 0.7982142857142858\n",
      "ep: 356, time: 0.058, train_loss: 0.35749850049614906, train_acc: 0.8837053571428571, test_loss: 0.5524700284004211, test_acc: 0.7982142857142858\n",
      "ep: 357, time: 0.071, train_loss: 0.3566942885518074, train_acc: 0.8841517857142858, test_loss: 0.5522449910640717, test_acc: 0.7982142857142858\n",
      "ep: 358, time: 0.065, train_loss: 0.35589030757546425, train_acc: 0.8845982142857143, test_loss: 0.5520220696926117, test_acc: 0.7982142857142858\n",
      "ep: 359, time: 0.069, train_loss: 0.3550865575671196, train_acc: 0.884375, test_loss: 0.551801323890686, test_acc: 0.7982142857142858\n",
      "ep: 360, time: 0.069, train_loss: 0.3542831167578697, train_acc: 0.884375, test_loss: 0.5515825748443604, test_acc: 0.7991071428571429\n",
      "ep: 361, time: 0.062, train_loss: 0.35347992181777954, train_acc: 0.8845982142857143, test_loss: 0.5513659119606018, test_acc: 0.7991071428571429\n",
      "ep: 362, time: 0.064, train_loss: 0.35267696157097816, train_acc: 0.8850446428571429, test_loss: 0.551151305437088, test_acc: 0.7991071428571429\n",
      "ep: 363, time: 0.087, train_loss: 0.3518742695450783, train_acc: 0.8850446428571429, test_loss: 0.5509388148784637, test_acc: 0.7982142857142858\n",
      "ep: 364, time: 0.078, train_loss: 0.3510718233883381, train_acc: 0.8859375, test_loss: 0.5507283210754395, test_acc: 0.7982142857142858\n",
      "ep: 365, time: 0.074, train_loss: 0.350269615650177, train_acc: 0.8859375, test_loss: 0.5505200326442719, test_acc: 0.7982142857142858\n",
      "ep: 366, time: 0.061, train_loss: 0.3494676500558853, train_acc: 0.8868303571428572, test_loss: 0.5503137707710266, test_acc: 0.7982142857142858\n",
      "ep: 367, time: 0.069, train_loss: 0.34866591542959213, train_acc: 0.8879464285714286, test_loss: 0.5501095056533813, test_acc: 0.7982142857142858\n",
      "ep: 368, time: 0.077, train_loss: 0.34786444157361984, train_acc: 0.8881696428571428, test_loss: 0.5499074757099152, test_acc: 0.7982142857142858\n",
      "ep: 369, time: 0.062, train_loss: 0.34706323221325874, train_acc: 0.8886160714285715, test_loss: 0.5497074127197266, test_acc: 0.7982142857142858\n",
      "ep: 370, time: 0.073, train_loss: 0.34626223891973495, train_acc: 0.8890625, test_loss: 0.5495094060897827, test_acc: 0.7982142857142858\n",
      "ep: 371, time: 0.074, train_loss: 0.34546150639653206, train_acc: 0.8892857142857142, test_loss: 0.5493136048316956, test_acc: 0.7973214285714286\n",
      "ep: 372, time: 0.066, train_loss: 0.34466099366545677, train_acc: 0.8899553571428571, test_loss: 0.5491200089454651, test_acc: 0.7964285714285714\n",
      "ep: 373, time: 0.068, train_loss: 0.34386079385876656, train_acc: 0.8904017857142857, test_loss: 0.5489283800125122, test_acc: 0.7964285714285714\n",
      "ep: 374, time: 0.074, train_loss: 0.34306079894304276, train_acc: 0.8908482142857143, test_loss: 0.5487390756607056, test_acc: 0.7964285714285714\n",
      "ep: 375, time: 0.063, train_loss: 0.34226108342409134, train_acc: 0.8908482142857143, test_loss: 0.5485517382621765, test_acc: 0.7964285714285714\n",
      "ep: 376, time: 0.071, train_loss: 0.3414616025984287, train_acc: 0.8908482142857143, test_loss: 0.5483664572238922, test_acc: 0.7964285714285714\n",
      "ep: 377, time: 0.062, train_loss: 0.3406623788177967, train_acc: 0.8912946428571429, test_loss: 0.5481833815574646, test_acc: 0.7964285714285714\n",
      "ep: 378, time: 0.069, train_loss: 0.3398634009063244, train_acc: 0.8912946428571429, test_loss: 0.5480024218559265, test_acc: 0.7964285714285714\n",
      "ep: 379, time: 0.073, train_loss: 0.33906471356749535, train_acc: 0.8917410714285714, test_loss: 0.5478236973285675, test_acc: 0.7973214285714286\n",
      "ep: 380, time: 0.077, train_loss: 0.3382662534713745, train_acc: 0.8924107142857143, test_loss: 0.5476470291614532, test_acc: 0.7973214285714286\n",
      "ep: 381, time: 0.074, train_loss: 0.33746806159615517, train_acc: 0.8930803571428572, test_loss: 0.5474725365638733, test_acc: 0.7973214285714286\n",
      "ep: 382, time: 0.064, train_loss: 0.3366701193153858, train_acc: 0.89375, test_loss: 0.5473001897335052, test_acc: 0.7973214285714286\n",
      "ep: 383, time: 0.059, train_loss: 0.33587241917848587, train_acc: 0.8939732142857143, test_loss: 0.5471300482749939, test_acc: 0.7973214285714286\n",
      "ep: 384, time: 0.072, train_loss: 0.3350750207901001, train_acc: 0.8941964285714286, test_loss: 0.5469620823860168, test_acc: 0.7973214285714286\n",
      "ep: 385, time: 0.074, train_loss: 0.3342778906226158, train_acc: 0.8946428571428572, test_loss: 0.5467962026596069, test_acc: 0.7964285714285714\n",
      "ep: 386, time: 0.059, train_loss: 0.3334810771048069, train_acc: 0.8946428571428572, test_loss: 0.5466326475143433, test_acc: 0.7964285714285714\n",
      "ep: 387, time: 0.057, train_loss: 0.3326845318078995, train_acc: 0.8955357142857143, test_loss: 0.5464712381362915, test_acc: 0.7964285714285714\n",
      "ep: 388, time: 0.069, train_loss: 0.33188824728131294, train_acc: 0.8957589285714286, test_loss: 0.5463120341300964, test_acc: 0.7964285714285714\n",
      "ep: 389, time: 0.082, train_loss: 0.3310922719538212, train_acc: 0.8962053571428571, test_loss: 0.5461549460887909, test_acc: 0.7982142857142858\n",
      "ep: 390, time: 0.064, train_loss: 0.3302965946495533, train_acc: 0.8962053571428571, test_loss: 0.5460001528263092, test_acc: 0.7982142857142858\n",
      "ep: 391, time: 0.067, train_loss: 0.3295012004673481, train_acc: 0.8966517857142857, test_loss: 0.5458475649356842, test_acc: 0.7982142857142858\n",
      "ep: 392, time: 0.072, train_loss: 0.32870613411068916, train_acc: 0.8973214285714286, test_loss: 0.5456973016262054, test_acc: 0.7982142857142858\n",
      "ep: 393, time: 0.066, train_loss: 0.3279113657772541, train_acc: 0.8975446428571429, test_loss: 0.5455491840839386, test_acc: 0.7982142857142858\n",
      "ep: 394, time: 0.073, train_loss: 0.32711688056588173, train_acc: 0.8982142857142857, test_loss: 0.5454033017158508, test_acc: 0.7982142857142858\n",
      "ep: 395, time: 0.073, train_loss: 0.3263227008283138, train_acc: 0.8988839285714286, test_loss: 0.5452596843242645, test_acc: 0.7982142857142858\n",
      "ep: 396, time: 0.064, train_loss: 0.3255288787186146, train_acc: 0.8997767857142858, test_loss: 0.5451183319091797, test_acc: 0.7982142857142858\n",
      "ep: 397, time: 0.073, train_loss: 0.324735376983881, train_acc: 0.9002232142857143, test_loss: 0.5449793338775635, test_acc: 0.7982142857142858\n",
      "ep: 398, time: 0.079, train_loss: 0.3239421844482422, train_acc: 0.9006696428571429, test_loss: 0.544842392206192, test_acc: 0.7982142857142858\n",
      "ep: 399, time: 0.071, train_loss: 0.3231493569910526, train_acc: 0.9008928571428572, test_loss: 0.5447078943252563, test_acc: 0.7982142857142858\n",
      "ep: 400, time: 0.061, train_loss: 0.32235684990882874, train_acc: 0.9013392857142857, test_loss: 0.5445756912231445, test_acc: 0.7991071428571429\n",
      "ep: 401, time: 0.079, train_loss: 0.3215646855533123, train_acc: 0.9015625, test_loss: 0.5444457232952118, test_acc: 0.7991071428571429\n",
      "ep: 402, time: 0.062, train_loss: 0.32077285647392273, train_acc: 0.9022321428571428, test_loss: 0.5443181097507477, test_acc: 0.7991071428571429\n",
      "ep: 403, time: 0.066, train_loss: 0.3199814185500145, train_acc: 0.9026785714285714, test_loss: 0.5441928505897522, test_acc: 0.7991071428571429\n",
      "ep: 404, time: 0.079, train_loss: 0.3191903680562973, train_acc: 0.9029017857142857, test_loss: 0.5440698862075806, test_acc: 0.7991071428571429\n",
      "ep: 405, time: 0.074, train_loss: 0.31839966401457787, train_acc: 0.9029017857142857, test_loss: 0.5439492762088776, test_acc: 0.8\n",
      "ep: 406, time: 0.069, train_loss: 0.31760935485363007, train_acc: 0.9035714285714286, test_loss: 0.5438310503959656, test_acc: 0.8\n",
      "ep: 407, time: 0.064, train_loss: 0.316819429397583, train_acc: 0.9035714285714286, test_loss: 0.5437151193618774, test_acc: 0.8\n",
      "ep: 408, time: 0.076, train_loss: 0.3160298764705658, train_acc: 0.9037946428571428, test_loss: 0.5436016023159027, test_acc: 0.8\n",
      "ep: 409, time: 0.067, train_loss: 0.3152407333254814, train_acc: 0.9044642857142857, test_loss: 0.5434904396533966, test_acc: 0.8008928571428572\n",
      "ep: 410, time: 0.073, train_loss: 0.3144519738852978, train_acc: 0.9046875, test_loss: 0.5433816313743591, test_acc: 0.8008928571428572\n",
      "ep: 411, time: 0.058, train_loss: 0.31366365030407906, train_acc: 0.9046875, test_loss: 0.5432752668857574, test_acc: 0.8008928571428572\n",
      "ep: 412, time: 0.076, train_loss: 0.31287575885653496, train_acc: 0.9053571428571429, test_loss: 0.5431713759899139, test_acc: 0.8008928571428572\n",
      "ep: 413, time: 0.074, train_loss: 0.3120882846415043, train_acc: 0.9053571428571429, test_loss: 0.5430698692798615, test_acc: 0.8017857142857143\n",
      "ep: 414, time: 0.078, train_loss: 0.31130122020840645, train_acc: 0.90625, test_loss: 0.5429707765579224, test_acc: 0.8017857142857143\n",
      "ep: 415, time: 0.070, train_loss: 0.31051456928253174, train_acc: 0.90625, test_loss: 0.5428741276264191, test_acc: 0.8017857142857143\n",
      "ep: 416, time: 0.071, train_loss: 0.30972839146852493, train_acc: 0.9064732142857143, test_loss: 0.542779952287674, test_acc: 0.8035714285714286\n",
      "ep: 417, time: 0.073, train_loss: 0.30894268304109573, train_acc: 0.9066964285714286, test_loss: 0.5426882803440094, test_acc: 0.8044642857142857\n",
      "ep: 418, time: 0.064, train_loss: 0.30815743654966354, train_acc: 0.9066964285714286, test_loss: 0.542599081993103, test_acc: 0.8044642857142857\n",
      "ep: 419, time: 0.072, train_loss: 0.30737267807126045, train_acc: 0.9075892857142858, test_loss: 0.5425123274326324, test_acc: 0.8044642857142857\n",
      "ep: 420, time: 0.071, train_loss: 0.3065883554518223, train_acc: 0.9078125, test_loss: 0.5424281656742096, test_acc: 0.8053571428571429\n",
      "ep: 421, time: 0.078, train_loss: 0.3058045357465744, train_acc: 0.9078125, test_loss: 0.5423465073108673, test_acc: 0.8053571428571429\n",
      "ep: 422, time: 0.073, train_loss: 0.3050212152302265, train_acc: 0.9082589285714285, test_loss: 0.5422672629356384, test_acc: 0.8053571428571429\n",
      "ep: 423, time: 0.080, train_loss: 0.30423836410045624, train_acc: 0.9089285714285714, test_loss: 0.542190670967102, test_acc: 0.8053571428571429\n",
      "ep: 424, time: 0.068, train_loss: 0.30345602706074715, train_acc: 0.9091517857142857, test_loss: 0.5421165227890015, test_acc: 0.8053571428571429\n",
      "ep: 425, time: 0.083, train_loss: 0.30267423391342163, train_acc: 0.9095982142857143, test_loss: 0.5420451760292053, test_acc: 0.8053571428571429\n",
      "ep: 426, time: 0.078, train_loss: 0.3018929772078991, train_acc: 0.9095982142857143, test_loss: 0.5419762432575226, test_acc: 0.8053571428571429\n",
      "ep: 427, time: 0.056, train_loss: 0.30111220851540565, train_acc: 0.9095982142857143, test_loss: 0.5419099628925323, test_acc: 0.8053571428571429\n",
      "ep: 428, time: 0.065, train_loss: 0.3003319948911667, train_acc: 0.9104910714285714, test_loss: 0.5418462753295898, test_acc: 0.8053571428571429\n",
      "ep: 429, time: 0.069, train_loss: 0.299552358686924, train_acc: 0.9109375, test_loss: 0.5417852699756622, test_acc: 0.80625\n",
      "ep: 430, time: 0.059, train_loss: 0.29877325519919395, train_acc: 0.9111607142857143, test_loss: 0.54172682762146, test_acc: 0.8053571428571429\n",
      "ep: 431, time: 0.075, train_loss: 0.2979947254061699, train_acc: 0.9116071428571428, test_loss: 0.5416710674762726, test_acc: 0.8053571428571429\n",
      "ep: 432, time: 0.078, train_loss: 0.297216784209013, train_acc: 0.9120535714285715, test_loss: 0.5416179001331329, test_acc: 0.8053571428571429\n",
      "ep: 433, time: 0.073, train_loss: 0.29643939808011055, train_acc: 0.9122767857142857, test_loss: 0.5415675342082977, test_acc: 0.8044642857142857\n",
      "ep: 434, time: 0.064, train_loss: 0.29566265270113945, train_acc: 0.9125, test_loss: 0.5415199100971222, test_acc: 0.8044642857142857\n",
      "ep: 435, time: 0.082, train_loss: 0.2948864921927452, train_acc: 0.9127232142857142, test_loss: 0.541474848985672, test_acc: 0.8044642857142857\n",
      "ep: 436, time: 0.079, train_loss: 0.2941109240055084, train_acc: 0.9131696428571429, test_loss: 0.541432648897171, test_acc: 0.8044642857142857\n",
      "ep: 437, time: 0.067, train_loss: 0.2933359816670418, train_acc: 0.9140625, test_loss: 0.5413931012153625, test_acc: 0.8053571428571429\n",
      "ep: 438, time: 0.057, train_loss: 0.2925616651773453, train_acc: 0.9142857142857143, test_loss: 0.5413563251495361, test_acc: 0.8053571428571429\n",
      "ep: 439, time: 0.078, train_loss: 0.2917880266904831, train_acc: 0.9149553571428571, test_loss: 0.5413223803043365, test_acc: 0.8053571428571429\n",
      "ep: 440, time: 0.066, train_loss: 0.29101501777768135, train_acc: 0.9151785714285714, test_loss: 0.541291207075119, test_acc: 0.8044642857142857\n",
      "ep: 441, time: 0.072, train_loss: 0.29024267196655273, train_acc: 0.915625, test_loss: 0.5412628650665283, test_acc: 0.8044642857142857\n",
      "ep: 442, time: 0.064, train_loss: 0.28947100043296814, train_acc: 0.9158482142857143, test_loss: 0.5412372052669525, test_acc: 0.8044642857142857\n",
      "ep: 443, time: 0.084, train_loss: 0.2886999845504761, train_acc: 0.9158482142857143, test_loss: 0.5412144958972931, test_acc: 0.8044642857142857\n",
      "ep: 444, time: 0.081, train_loss: 0.2879296615719795, train_acc: 0.9162946428571429, test_loss: 0.5411946177482605, test_acc: 0.8044642857142857\n",
      "ep: 445, time: 0.063, train_loss: 0.2871600575745106, train_acc: 0.9167410714285714, test_loss: 0.5411776602268219, test_acc: 0.8044642857142857\n",
      "ep: 446, time: 0.070, train_loss: 0.28639112040400505, train_acc: 0.9176339285714286, test_loss: 0.541163444519043, test_acc: 0.8044642857142857\n",
      "ep: 447, time: 0.066, train_loss: 0.2856229245662689, train_acc: 0.9180803571428572, test_loss: 0.5411522090435028, test_acc: 0.8044642857142857\n",
      "ep: 448, time: 0.067, train_loss: 0.28485546447336674, train_acc: 0.9189732142857143, test_loss: 0.5411439538002014, test_acc: 0.8044642857142857\n",
      "ep: 449, time: 0.067, train_loss: 0.2840887401252985, train_acc: 0.9191964285714286, test_loss: 0.5411385297775269, test_acc: 0.8044642857142857\n",
      "ep: 450, time: 0.072, train_loss: 0.28332276456058025, train_acc: 0.9191964285714286, test_loss: 0.5411360561847687, test_acc: 0.8044642857142857\n",
      "ep: 451, time: 0.068, train_loss: 0.28255753219127655, train_acc: 0.9194196428571428, test_loss: 0.541136622428894, test_acc: 0.8035714285714286\n",
      "ep: 452, time: 0.064, train_loss: 0.2817930728197098, train_acc: 0.9198660714285715, test_loss: 0.5411399602890015, test_acc: 0.8035714285714286\n",
      "ep: 453, time: 0.060, train_loss: 0.2810294032096863, train_acc: 0.9200892857142857, test_loss: 0.541146457195282, test_acc: 0.8035714285714286\n",
      "ep: 454, time: 0.064, train_loss: 0.28026649728417397, train_acc: 0.9203125, test_loss: 0.5411559343338013, test_acc: 0.8044642857142857\n",
      "ep: 455, time: 0.061, train_loss: 0.27950440533459187, train_acc: 0.9205357142857142, test_loss: 0.5411684513092041, test_acc: 0.8044642857142857\n",
      "ep: 456, time: 0.062, train_loss: 0.2787431478500366, train_acc: 0.9207589285714286, test_loss: 0.5411839783191681, test_acc: 0.8044642857142857\n",
      "ep: 457, time: 0.072, train_loss: 0.2779827043414116, train_acc: 0.9214285714285714, test_loss: 0.5412024855613708, test_acc: 0.8044642857142857\n",
      "ep: 458, time: 0.066, train_loss: 0.27722309716045856, train_acc: 0.9214285714285714, test_loss: 0.5412242114543915, test_acc: 0.8044642857142857\n",
      "ep: 459, time: 0.075, train_loss: 0.2764643542468548, train_acc: 0.921875, test_loss: 0.5412488877773285, test_acc: 0.8044642857142857\n",
      "ep: 460, time: 0.068, train_loss: 0.27570641972124577, train_acc: 0.921875, test_loss: 0.5412766635417938, test_acc: 0.8044642857142857\n",
      "ep: 461, time: 0.062, train_loss: 0.27494938112795353, train_acc: 0.9223214285714286, test_loss: 0.5413076281547546, test_acc: 0.8044642857142857\n",
      "ep: 462, time: 0.077, train_loss: 0.27419321052730083, train_acc: 0.9229910714285714, test_loss: 0.5413416028022766, test_acc: 0.8044642857142857\n",
      "ep: 463, time: 0.078, train_loss: 0.27343791350722313, train_acc: 0.9232142857142858, test_loss: 0.5413788259029388, test_acc: 0.8044642857142857\n",
      "ep: 464, time: 0.064, train_loss: 0.2726835235953331, train_acc: 0.9236607142857143, test_loss: 0.541419118642807, test_acc: 0.8035714285714286\n",
      "ep: 465, time: 0.073, train_loss: 0.27193007804453373, train_acc: 0.9238839285714285, test_loss: 0.5414626598358154, test_acc: 0.8035714285714286\n",
      "ep: 466, time: 0.080, train_loss: 0.27117753587663174, train_acc: 0.9241071428571429, test_loss: 0.5415092408657074, test_acc: 0.8035714285714286\n",
      "ep: 467, time: 0.059, train_loss: 0.27042591385543346, train_acc: 0.9243303571428572, test_loss: 0.5415591895580292, test_acc: 0.8026785714285715\n",
      "ep: 468, time: 0.072, train_loss: 0.26967527717351913, train_acc: 0.9247767857142857, test_loss: 0.5416122376918793, test_acc: 0.8017857142857143\n",
      "ep: 469, time: 0.075, train_loss: 0.2689255401492119, train_acc: 0.925, test_loss: 0.541668564081192, test_acc: 0.8017857142857143\n",
      "ep: 470, time: 0.076, train_loss: 0.26817680336534977, train_acc: 0.9254464285714286, test_loss: 0.5417280197143555, test_acc: 0.8017857142857143\n",
      "ep: 471, time: 0.077, train_loss: 0.2674290407449007, train_acc: 0.9256696428571428, test_loss: 0.5417907536029816, test_acc: 0.8017857142857143\n",
      "ep: 472, time: 0.073, train_loss: 0.26668223179876804, train_acc: 0.9258928571428572, test_loss: 0.5418568253517151, test_acc: 0.8017857142857143\n",
      "ep: 473, time: 0.057, train_loss: 0.2659364715218544, train_acc: 0.9261160714285714, test_loss: 0.5419261157512665, test_acc: 0.8017857142857143\n",
      "ep: 474, time: 0.059, train_loss: 0.2651917301118374, train_acc: 0.9267857142857143, test_loss: 0.5419986546039581, test_acc: 0.8017857142857143\n",
      "ep: 475, time: 0.064, train_loss: 0.26444801315665245, train_acc: 0.9270089285714286, test_loss: 0.5420744121074677, test_acc: 0.8017857142857143\n",
      "ep: 476, time: 0.059, train_loss: 0.2637053169310093, train_acc: 0.9272321428571428, test_loss: 0.5421535968780518, test_acc: 0.8017857142857143\n",
      "ep: 477, time: 0.069, train_loss: 0.26296367309987545, train_acc: 0.9272321428571428, test_loss: 0.5422360599040985, test_acc: 0.8017857142857143\n",
      "ep: 478, time: 0.073, train_loss: 0.26222311705350876, train_acc: 0.9274553571428571, test_loss: 0.5423218607902527, test_acc: 0.8017857142857143\n",
      "ep: 479, time: 0.075, train_loss: 0.2614835985004902, train_acc: 0.9274553571428571, test_loss: 0.5424109697341919, test_acc: 0.8008928571428572\n",
      "ep: 480, time: 0.068, train_loss: 0.26074517145752907, train_acc: 0.9274553571428571, test_loss: 0.5425034165382385, test_acc: 0.8008928571428572\n",
      "ep: 481, time: 0.078, train_loss: 0.26000782288610935, train_acc: 0.928125, test_loss: 0.5425991714000702, test_acc: 0.8017857142857143\n",
      "ep: 482, time: 0.060, train_loss: 0.25927161797881126, train_acc: 0.9283482142857142, test_loss: 0.5426983535289764, test_acc: 0.8017857142857143\n",
      "ep: 483, time: 0.061, train_loss: 0.25853652134537697, train_acc: 0.9283482142857142, test_loss: 0.5428008437156677, test_acc: 0.8017857142857143\n",
      "ep: 484, time: 0.064, train_loss: 0.25780255533754826, train_acc: 0.9285714285714286, test_loss: 0.5429066717624664, test_acc: 0.8017857142857143\n",
      "ep: 485, time: 0.072, train_loss: 0.25706975534558296, train_acc: 0.9285714285714286, test_loss: 0.5430159568786621, test_acc: 0.8017857142857143\n",
      "ep: 486, time: 0.081, train_loss: 0.2563380766659975, train_acc: 0.9287946428571429, test_loss: 0.5431285500526428, test_acc: 0.8017857142857143\n",
      "ep: 487, time: 0.081, train_loss: 0.25560758262872696, train_acc: 0.9294642857142857, test_loss: 0.5432446002960205, test_acc: 0.8017857142857143\n",
      "ep: 488, time: 0.065, train_loss: 0.25487823598086834, train_acc: 0.9294642857142857, test_loss: 0.5433640480041504, test_acc: 0.8017857142857143\n",
      "ep: 489, time: 0.064, train_loss: 0.25415011681616306, train_acc: 0.9292410714285714, test_loss: 0.5434868037700653, test_acc: 0.8026785714285715\n",
      "ep: 490, time: 0.073, train_loss: 0.25342315435409546, train_acc: 0.9296875, test_loss: 0.5436129868030548, test_acc: 0.8026785714285715\n",
      "ep: 491, time: 0.068, train_loss: 0.25269738398492336, train_acc: 0.9299107142857143, test_loss: 0.5437425374984741, test_acc: 0.8026785714285715\n",
      "ep: 492, time: 0.074, train_loss: 0.25197288393974304, train_acc: 0.9305803571428571, test_loss: 0.5438755750656128, test_acc: 0.8026785714285715\n",
      "ep: 493, time: 0.056, train_loss: 0.2512495983392, train_acc: 0.9308035714285714, test_loss: 0.5440120697021484, test_acc: 0.8026785714285715\n",
      "ep: 494, time: 0.060, train_loss: 0.25052759796380997, train_acc: 0.9308035714285714, test_loss: 0.5441518425941467, test_acc: 0.8026785714285715\n",
      "ep: 495, time: 0.063, train_loss: 0.2498068381100893, train_acc: 0.9310267857142858, test_loss: 0.5442951321601868, test_acc: 0.8026785714285715\n",
      "ep: 496, time: 0.075, train_loss: 0.24908734299242496, train_acc: 0.9316964285714285, test_loss: 0.544441819190979, test_acc: 0.8026785714285715\n",
      "ep: 497, time: 0.081, train_loss: 0.248369125649333, train_acc: 0.9323660714285714, test_loss: 0.5445919632911682, test_acc: 0.8017857142857143\n",
      "ep: 498, time: 0.063, train_loss: 0.2476521972566843, train_acc: 0.9325892857142857, test_loss: 0.5447454154491425, test_acc: 0.8017857142857143\n",
      "ep: 499, time: 0.059, train_loss: 0.24693656153976917, train_acc: 0.9328125, test_loss: 0.5449022948741913, test_acc: 0.8017857142857143\n"
     ]
    }
   ],
   "source": [
    "model = Network()\n",
    "train_model(model, 500, XR, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoded: ртн згзг хдрящ ыихцсящ уфдёмп\n",
      "original: мой дядя самых честных правил\n",
      "decoded: лои бядя оамыт черрныт ноааил\n",
      "=====\n",
      "encoded: пузие тй ж эшчпш метйсуз\n",
      "original: когда не в шутку занемог\n",
      "decoded: илгда не б чттис занемог\n",
      "=====\n",
      "encoded: ут шжелечб цйёд мецчежнр\n",
      "original: он уважать себя заставил\n",
      "decoded: нн свадать свбя еасравил\n",
      "=====\n",
      "encoded: л оцъыз еюжцпгхя рз псё\n",
      "original: и лучше выдумать не мог\n",
      "decoded: и лсчче быдулать не мог\n",
      "=====\n",
      "encoded: ижт уфмриф зфчжмр сдчод\n",
      "original: его пример другим наука\n",
      "decoded: вао ноимер вотвим натиа\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'encoded: {encoded_rand[i]}\\noriginal: {data[i]}\\ndecoded: {decode(encoded[i])}\\n=====')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** в случае со случайным сдвигом букв (от 2 до 5) модель показывает более низкое качество. Это обусловлено тем, что сдвиг случаен и модель не может найти однозначную взаимосвязь между входными и выходными данными. Если бы каждая буква сдвигалась каждый раз на один и тот же шаг, то модель вполне могла значительно большей точности, так как была бы явная связь между \"зашифрованными\" и оригинальными символами. Вероятно, точность модели будет снижаться с ростом размера данных, т.к. связь между входными и выходными данными будет все менее явной."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a00e7ec61eff5a88fc1f6f522f8352e0b0489450640f58684f8d2503099540c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
