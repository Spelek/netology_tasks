{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Домашнее задание №6. Рекуррентные сети 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Генерация данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from random import randint\n",
    "\n",
    "N = 20\n",
    "M = 1000\n",
    "K = int(M * 0.8)\n",
    "\n",
    "x_train, x_test = torch.zeros(K, N, dtype=int), torch.zeros(M - K, N, dtype=int)\n",
    "y_train, y_test = torch.zeros(K, N, dtype=int), torch.zeros(M - K, N, dtype=int)\n",
    "\n",
    "for j in range(M):\n",
    "    X, Y = torch.zeros(N, dtype=int), torch.zeros(N, dtype=int)\n",
    "    for i in range(N):\n",
    "        X[i] = randint(0, 9)\n",
    "        if i == 0:\n",
    "            Y[i] = X[i]\n",
    "        else:\n",
    "            Y[i] = X[i] + X[0]\n",
    "            if Y[i] >= 10:\n",
    "                Y[i] -= 10\n",
    "    if j < K:\n",
    "        x_train[j] = X\n",
    "        y_train[j] = Y\n",
    "    else:\n",
    "        x_test[j - K] = X\n",
    "        y_test[j - K] = Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([800, 20]), torch.Size([200, 20]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "def train_model(model, num_epochs, batch_size, data, target):\n",
    "\n",
    "    train, test = data[0], data[1]\n",
    "    y_train, y_test = target[0], target[1]\n",
    "\n",
    "    loss = torch.nn.CrossEntropyLoss()\n",
    "    trainer = torch.optim.Adam(model.parameters(), lr=.001)\n",
    "\n",
    "    for ep in range(num_epochs):\n",
    "        \n",
    "        train_iters, train_passed  = 0, 0\n",
    "        train_loss, train_acc = 0., 0.\n",
    "        start=time.time()\n",
    "        \n",
    "        model.train()\n",
    "        for i in range(int(len(train) / batch_size)):\n",
    "            X_batch = train[i * batch_size:(i + 1) * batch_size]\n",
    "            Y_batch = y_train[i * batch_size:(i + 1) * batch_size].flatten()\n",
    "            trainer.zero_grad()\n",
    "            y_pred = model.forward(X_batch)\n",
    "            y_pred = y_pred.view(-1, 10)\n",
    "            l = loss(y_pred, Y_batch)\n",
    "            l.backward()\n",
    "            trainer.step()\n",
    "            train_loss += l.item()\n",
    "            train_acc += (y_pred.argmax(dim=1) == Y_batch).sum().item()\n",
    "            train_iters += 1\n",
    "            train_passed += Y_batch.shape[0]\n",
    "        \n",
    "        test_iters, test_passed  = 0, 0\n",
    "        test_loss, test_acc = 0., 0.\n",
    "        for i in range(int(len(test) / batch_size)):\n",
    "            X_batch = test[i * batch_size:(i + 1) * batch_size]\n",
    "            Y_batch = y_test[i * batch_size:(i + 1) * batch_size].flatten()\n",
    "            y_pred = model(X_batch).view(-1, 10)\n",
    "            l = loss(y_pred, Y_batch)\n",
    "            test_loss += l.item()\n",
    "            test_acc += (y_pred.argmax(dim=1) == Y_batch).sum().item()\n",
    "            test_iters += 1\n",
    "            test_passed += Y_batch.shape[0]\n",
    "            \n",
    "        train_acc_res = train_acc / train_passed\n",
    "        test_acc_res = test_acc / test_passed\n",
    "        print(\"ep: {}, time: {:.3f}, train_loss: {}, train_acc: {}, test_loss: {}, test_acc: {}\".format(\n",
    "            ep, time.time() - start, train_loss / train_iters, train_acc_res,\n",
    "            test_loss / test_iters, test_acc_res)\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Модели"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(RNN, self).__init__()\n",
    "        self.embedding = torch.nn.Embedding(10, 20)\n",
    "        self.rnn = torch.nn.RNN(20, 128, batch_first=True)\n",
    "        self.linear = torch.nn.Linear(128, 10)\n",
    "        \n",
    "    def forward(self, seq, state=None):\n",
    "        embed = self.embedding(seq)\n",
    "        o, s = self.rnn(embed)\n",
    "        out = self.linear(o)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, time: 0.339, train_loss: 2.3097122609615326, train_acc: 0.113375, test_loss: 2.3002854108810427, test_acc: 0.13425\n",
      "ep: 1, time: 0.221, train_loss: 2.294386678934097, train_acc: 0.1389375, test_loss: 2.3018823146820067, test_acc: 0.1395\n",
      "ep: 2, time: 0.277, train_loss: 2.291664016246796, train_acc: 0.1433125, test_loss: 2.302675747871399, test_acc: 0.1455\n",
      "ep: 3, time: 0.253, train_loss: 2.2895939648151398, train_acc: 0.1460625, test_loss: 2.30360586643219, test_acc: 0.146\n",
      "ep: 4, time: 0.278, train_loss: 2.287821626663208, train_acc: 0.1488125, test_loss: 2.304696869850159, test_acc: 0.1485\n",
      "ep: 5, time: 0.218, train_loss: 2.2862353146076204, train_acc: 0.1506875, test_loss: 2.305756115913391, test_acc: 0.14925\n",
      "ep: 6, time: 0.227, train_loss: 2.2846920788288116, train_acc: 0.153, test_loss: 2.306724500656128, test_acc: 0.1505\n",
      "ep: 7, time: 0.240, train_loss: 2.2830352604389192, train_acc: 0.1551875, test_loss: 2.3074434995651245, test_acc: 0.15125\n",
      "ep: 8, time: 0.227, train_loss: 2.281010776758194, train_acc: 0.157125, test_loss: 2.3074021339416504, test_acc: 0.152\n",
      "ep: 9, time: 0.230, train_loss: 2.276460474729538, train_acc: 0.1580625, test_loss: 2.2994996547698974, test_acc: 0.15075\n",
      "ep: 10, time: 0.259, train_loss: 2.2700537919998167, train_acc: 0.162, test_loss: 2.2903210163116454, test_acc: 0.1535\n",
      "ep: 11, time: 0.216, train_loss: 2.2597023487091064, train_acc: 0.16475, test_loss: 2.2791943550109863, test_acc: 0.15625\n",
      "ep: 12, time: 0.220, train_loss: 2.253082150220871, train_acc: 0.169625, test_loss: 2.2854862928390505, test_acc: 0.15925\n",
      "ep: 13, time: 0.206, train_loss: 2.246479278802872, train_acc: 0.1761875, test_loss: 2.2488992691040037, test_acc: 0.1825\n",
      "ep: 14, time: 0.232, train_loss: 2.2113146841526032, train_acc: 0.19925, test_loss: 2.22677481174469, test_acc: 0.2015\n",
      "ep: 15, time: 0.204, train_loss: 2.1654391407966616, train_acc: 0.21325, test_loss: 2.1596405506134033, test_acc: 0.21775\n",
      "ep: 16, time: 0.227, train_loss: 2.1056443840265273, train_acc: 0.2261875, test_loss: 2.0945012092590334, test_acc: 0.233\n",
      "ep: 17, time: 0.200, train_loss: 2.031317323446274, train_acc: 0.2354375, test_loss: 2.0094428300857543, test_acc: 0.24575\n",
      "ep: 18, time: 0.204, train_loss: 1.9599367141723634, train_acc: 0.248625, test_loss: 1.9503608703613282, test_acc: 0.261\n",
      "ep: 19, time: 0.209, train_loss: 1.9045128375291824, train_acc: 0.260125, test_loss: 1.9221182465553284, test_acc: 0.25575\n",
      "ep: 20, time: 0.229, train_loss: 1.8536632657051086, train_acc: 0.2709375, test_loss: 1.8247878551483154, test_acc: 0.2755\n",
      "ep: 21, time: 0.177, train_loss: 1.7675756961107254, train_acc: 0.3056875, test_loss: 1.766890048980713, test_acc: 0.3205\n",
      "ep: 22, time: 0.196, train_loss: 1.6640547513961792, train_acc: 0.3418125, test_loss: 1.8409774899482727, test_acc: 0.30275\n",
      "ep: 23, time: 0.183, train_loss: 1.6260469138622284, train_acc: 0.355625, test_loss: 1.6098711490631104, test_acc: 0.345\n",
      "ep: 24, time: 0.210, train_loss: 1.5398974746465683, train_acc: 0.3605, test_loss: 1.5515889167785644, test_acc: 0.35925\n",
      "ep: 25, time: 0.190, train_loss: 1.4743025660514832, train_acc: 0.3811875, test_loss: 1.4778994083404542, test_acc: 0.39475\n",
      "ep: 26, time: 0.216, train_loss: 1.3969489276409148, train_acc: 0.435375, test_loss: 1.401099967956543, test_acc: 0.441\n",
      "ep: 27, time: 0.201, train_loss: 1.3319527357816696, train_acc: 0.4566875, test_loss: 1.3549753904342652, test_acc: 0.4415\n",
      "ep: 28, time: 0.216, train_loss: 1.3213914215564728, train_acc: 0.4576875, test_loss: 1.476735520362854, test_acc: 0.4135\n",
      "ep: 29, time: 0.179, train_loss: 1.35133239030838, train_acc: 0.446625, test_loss: 1.3491486430168151, test_acc: 0.43375\n",
      "ep: 30, time: 0.197, train_loss: 1.2432882487773895, train_acc: 0.465875, test_loss: 1.2869699478149415, test_acc: 0.44275\n",
      "ep: 31, time: 0.193, train_loss: 1.2003222197294234, train_acc: 0.47325, test_loss: 1.238020646572113, test_acc: 0.447\n",
      "ep: 32, time: 0.184, train_loss: 1.1763301461935043, train_acc: 0.4785, test_loss: 1.240649938583374, test_acc: 0.4445\n",
      "ep: 33, time: 0.230, train_loss: 1.1755266204476356, train_acc: 0.472375, test_loss: 1.2207524180412292, test_acc: 0.43675\n",
      "ep: 34, time: 0.205, train_loss: 1.1525887981057168, train_acc: 0.4775, test_loss: 1.1990844130516052, test_acc: 0.44275\n",
      "ep: 35, time: 0.206, train_loss: 1.1362738579511642, train_acc: 0.485, test_loss: 1.186434268951416, test_acc: 0.44725\n",
      "ep: 36, time: 0.182, train_loss: 1.1243112832307816, train_acc: 0.48875, test_loss: 1.1758600175380707, test_acc: 0.44975\n",
      "ep: 37, time: 0.212, train_loss: 1.114197014272213, train_acc: 0.4901875, test_loss: 1.1671511352062225, test_acc: 0.449\n",
      "ep: 38, time: 0.209, train_loss: 1.1049362942576408, train_acc: 0.4930625, test_loss: 1.1589974880218505, test_acc: 0.4515\n",
      "ep: 39, time: 0.194, train_loss: 1.096635590493679, train_acc: 0.495375, test_loss: 1.1515245020389557, test_acc: 0.45225\n",
      "ep: 40, time: 0.179, train_loss: 1.0916705906391144, train_acc: 0.498, test_loss: 1.145927518606186, test_acc: 0.45525\n",
      "ep: 41, time: 0.182, train_loss: 1.0841678142547608, train_acc: 0.5006875, test_loss: 1.138244193792343, test_acc: 0.45975\n",
      "ep: 42, time: 0.214, train_loss: 1.0769772246479987, train_acc: 0.5035625, test_loss: 1.1327447474002839, test_acc: 0.45925\n",
      "ep: 43, time: 0.222, train_loss: 1.0711239233613015, train_acc: 0.5093125, test_loss: 1.1411091029644012, test_acc: 0.4745\n",
      "ep: 44, time: 0.207, train_loss: 1.0688420951366424, train_acc: 0.515625, test_loss: 1.1168650329113006, test_acc: 0.48325\n",
      "ep: 45, time: 0.189, train_loss: 1.0963448956608772, train_acc: 0.5079375, test_loss: 1.1209110856056212, test_acc: 0.4675\n",
      "ep: 46, time: 0.207, train_loss: 1.057641114294529, train_acc: 0.5188125, test_loss: 1.1003286302089692, test_acc: 0.48975\n",
      "ep: 47, time: 0.192, train_loss: 1.0186598882079125, train_acc: 0.5585625, test_loss: 1.0583208084106446, test_acc: 0.526\n",
      "ep: 48, time: 0.232, train_loss: 0.9780993282794952, train_acc: 0.57525, test_loss: 1.0321202516555785, test_acc: 0.52675\n",
      "ep: 49, time: 0.210, train_loss: 0.9559079810976983, train_acc: 0.5794375, test_loss: 1.0174717783927918, test_acc: 0.5295\n",
      "ep: 50, time: 0.217, train_loss: 0.9421536818146705, train_acc: 0.580875, test_loss: 1.00575390458107, test_acc: 0.53325\n",
      "ep: 51, time: 0.186, train_loss: 0.9309684947133064, train_acc: 0.5833125, test_loss: 0.9959966838359833, test_acc: 0.533\n",
      "ep: 52, time: 0.228, train_loss: 0.9213706105947495, train_acc: 0.585625, test_loss: 0.9873836040496826, test_acc: 0.53275\n",
      "ep: 53, time: 0.229, train_loss: 1.060710969567299, train_acc: 0.552125, test_loss: 1.4946343779563904, test_acc: 0.4185\n",
      "ep: 54, time: 0.233, train_loss: 1.3288625434041024, train_acc: 0.4656875, test_loss: 1.3143630146980285, test_acc: 0.45725\n",
      "ep: 55, time: 0.217, train_loss: 1.2260332852602005, train_acc: 0.47425, test_loss: 1.246417796611786, test_acc: 0.4635\n",
      "ep: 56, time: 0.217, train_loss: 1.1750875115394592, train_acc: 0.4881875, test_loss: 1.2113899827003478, test_acc: 0.47225\n",
      "ep: 57, time: 0.251, train_loss: 1.153189368546009, train_acc: 0.4981875, test_loss: 1.171318280696869, test_acc: 0.4735\n",
      "ep: 58, time: 0.234, train_loss: 1.1099300757050514, train_acc: 0.499625, test_loss: 1.1369060575962067, test_acc: 0.47575\n",
      "ep: 59, time: 0.235, train_loss: 1.0745424911379815, train_acc: 0.5096875, test_loss: 1.128587508201599, test_acc: 0.48025\n",
      "ep: 60, time: 0.218, train_loss: 1.0948159635066985, train_acc: 0.508625, test_loss: 1.2666138291358948, test_acc: 0.44975\n",
      "ep: 61, time: 0.237, train_loss: 1.147385983169079, train_acc: 0.4854375, test_loss: 1.1325188398361206, test_acc: 0.47675\n",
      "ep: 62, time: 0.204, train_loss: 1.0770450696349143, train_acc: 0.4988125, test_loss: 1.112484985589981, test_acc: 0.47525\n",
      "ep: 63, time: 0.203, train_loss: 1.0582905068993569, train_acc: 0.503625, test_loss: 1.1012101650238038, test_acc: 0.47375\n",
      "ep: 64, time: 0.244, train_loss: 1.0475773125886918, train_acc: 0.5080625, test_loss: 1.0921328604221343, test_acc: 0.4835\n",
      "ep: 65, time: 0.184, train_loss: 1.0384345561265946, train_acc: 0.512375, test_loss: 1.080945187807083, test_acc: 0.48925\n",
      "ep: 66, time: 0.218, train_loss: 1.0269089087843895, train_acc: 0.5186875, test_loss: 1.069903951883316, test_acc: 0.4985\n",
      "ep: 67, time: 0.187, train_loss: 0.992970560491085, train_acc: 0.538, test_loss: 1.001379132270813, test_acc: 0.53225\n",
      "ep: 68, time: 0.212, train_loss: 0.9148114129900933, train_acc: 0.574, test_loss: 0.907718974351883, test_acc: 0.56625\n",
      "ep: 69, time: 0.178, train_loss: 0.8584445089101791, train_acc: 0.58975, test_loss: 0.8782290875911712, test_acc: 0.56975\n",
      "ep: 70, time: 0.177, train_loss: 0.8329949170351029, train_acc: 0.5950625, test_loss: 0.8571730315685272, test_acc: 0.57125\n",
      "ep: 71, time: 0.192, train_loss: 0.8138954862952232, train_acc: 0.597875, test_loss: 0.8407853901386261, test_acc: 0.56875\n",
      "ep: 72, time: 0.227, train_loss: 0.79895800948143, train_acc: 0.5998125, test_loss: 0.8277939677238464, test_acc: 0.56675\n",
      "ep: 73, time: 0.212, train_loss: 0.7869592353701591, train_acc: 0.6018125, test_loss: 0.816898375749588, test_acc: 0.5695\n",
      "ep: 74, time: 0.235, train_loss: 0.776827073097229, train_acc: 0.6026875, test_loss: 0.8075205266475678, test_acc: 0.5705\n",
      "ep: 75, time: 0.259, train_loss: 0.7680108442902565, train_acc: 0.60575, test_loss: 0.7993472874164581, test_acc: 0.57175\n",
      "ep: 76, time: 0.251, train_loss: 0.7601233959197998, train_acc: 0.607625, test_loss: 0.7943589389324188, test_acc: 0.57325\n",
      "ep: 77, time: 0.239, train_loss: 0.7528806358575821, train_acc: 0.60875, test_loss: 0.789030659198761, test_acc: 0.57325\n",
      "ep: 78, time: 0.208, train_loss: 0.7462628901004791, train_acc: 0.6115, test_loss: 0.7842079102993011, test_acc: 0.579\n",
      "ep: 79, time: 0.229, train_loss: 0.7411902233958244, train_acc: 0.6129375, test_loss: 0.7801244139671326, test_acc: 0.578\n",
      "ep: 80, time: 0.249, train_loss: 0.7341642990708351, train_acc: 0.6169375, test_loss: 0.7719412803649902, test_acc: 0.58575\n",
      "ep: 81, time: 0.240, train_loss: 0.7274088725447655, train_acc: 0.62175, test_loss: 0.7613026142120362, test_acc: 0.595\n",
      "ep: 82, time: 0.261, train_loss: 0.7190763801336288, train_acc: 0.6300625, test_loss: 0.7425870895385742, test_acc: 0.6215\n",
      "ep: 83, time: 0.249, train_loss: 0.700775383412838, train_acc: 0.6500625, test_loss: 0.7075214803218841, test_acc: 0.6565\n",
      "ep: 84, time: 0.263, train_loss: 0.6589788146317005, train_acc: 0.6875, test_loss: 0.6611604392528534, test_acc: 0.70025\n",
      "ep: 85, time: 0.219, train_loss: 0.6083071686327457, train_acc: 0.7265, test_loss: 0.6228989720344543, test_acc: 0.70925\n",
      "ep: 86, time: 0.207, train_loss: 0.5792091511189937, train_acc: 0.745, test_loss: 0.5894136309623719, test_acc: 0.731\n",
      "ep: 87, time: 0.217, train_loss: 0.5493282385170459, train_acc: 0.7590625, test_loss: 0.5724675804376602, test_acc: 0.74525\n",
      "ep: 88, time: 0.205, train_loss: 0.5205878965556622, train_acc: 0.774, test_loss: 0.5257142901420593, test_acc: 0.76375\n",
      "ep: 89, time: 0.224, train_loss: 0.5004823766648769, train_acc: 0.7771875, test_loss: 0.5207601696252823, test_acc: 0.7625\n",
      "ep: 90, time: 0.193, train_loss: 0.5030513674020767, train_acc: 0.7758125, test_loss: 0.5371931910514831, test_acc: 0.7575\n",
      "ep: 91, time: 0.223, train_loss: 0.49821553379297256, train_acc: 0.78, test_loss: 0.49551586508750917, test_acc: 0.7665\n",
      "ep: 92, time: 0.212, train_loss: 0.46946585476398467, train_acc: 0.7888125, test_loss: 0.48119704723358153, test_acc: 0.7715\n",
      "ep: 93, time: 0.208, train_loss: 0.45797487571835516, train_acc: 0.795625, test_loss: 0.46056629419326783, test_acc: 0.78825\n",
      "ep: 94, time: 0.205, train_loss: 0.45019475296139716, train_acc: 0.803875, test_loss: 0.44004209339618683, test_acc: 0.8095\n",
      "ep: 95, time: 0.212, train_loss: 0.4133377932012081, train_acc: 0.8400625, test_loss: 0.381341478228569, test_acc: 0.86825\n",
      "ep: 96, time: 0.221, train_loss: 0.342311729863286, train_acc: 0.899625, test_loss: 0.31622724384069445, test_acc: 0.918\n",
      "ep: 97, time: 0.187, train_loss: 0.284288689494133, train_acc: 0.9316875, test_loss: 0.2823938444256783, test_acc: 0.92175\n",
      "ep: 98, time: 0.212, train_loss: 0.25084104761481285, train_acc: 0.9358125, test_loss: 0.2577148973941803, test_acc: 0.92625\n",
      "ep: 99, time: 0.182, train_loss: 0.23020171970129014, train_acc: 0.937875, test_loss: 0.24027392864227295, test_acc: 0.92725\n",
      "ep: 100, time: 0.192, train_loss: 0.2141122531145811, train_acc: 0.9408125, test_loss: 0.22574579119682311, test_acc: 0.932\n",
      "ep: 101, time: 0.199, train_loss: 0.2005416002124548, train_acc: 0.944875, test_loss: 0.21253526359796523, test_acc: 0.93525\n",
      "ep: 102, time: 0.214, train_loss: 0.18815049584954976, train_acc: 0.9505, test_loss: 0.19734790027141572, test_acc: 0.94775\n",
      "ep: 103, time: 0.203, train_loss: 0.17562154158949853, train_acc: 0.9596875, test_loss: 0.17364056780934334, test_acc: 0.96775\n",
      "ep: 104, time: 0.201, train_loss: 0.15764105934649705, train_acc: 0.97725, test_loss: 0.21316244527697564, test_acc: 0.94775\n",
      "ep: 105, time: 0.203, train_loss: 0.12955953925848007, train_acc: 0.991375, test_loss: 0.11695187389850617, test_acc: 0.998\n",
      "ep: 106, time: 0.195, train_loss: 0.1018341576680541, train_acc: 0.999625, test_loss: 0.10198535323143006, test_acc: 0.99925\n",
      "ep: 107, time: 0.217, train_loss: 0.09019133802503347, train_acc: 0.999875, test_loss: 0.09221647456288337, test_acc: 0.9995\n",
      "ep: 108, time: 0.202, train_loss: 0.08196853846311569, train_acc: 0.999875, test_loss: 0.08442042693495751, test_acc: 0.9995\n",
      "ep: 109, time: 0.193, train_loss: 0.07516395878046751, train_acc: 0.999875, test_loss: 0.07790828421711922, test_acc: 0.9995\n",
      "ep: 110, time: 0.181, train_loss: 0.06937061753123999, train_acc: 0.999875, test_loss: 0.07224597111344337, test_acc: 0.9995\n",
      "ep: 111, time: 0.211, train_loss: 0.06434757700189948, train_acc: 0.999875, test_loss: 0.06723791882395744, test_acc: 1.0\n",
      "ep: 112, time: 0.196, train_loss: 0.059933877363801, train_acc: 1.0, test_loss: 0.06273613013327121, test_acc: 1.0\n",
      "ep: 113, time: 0.209, train_loss: 0.05601784037426114, train_acc: 1.0, test_loss: 0.0586519755423069, test_acc: 1.0\n",
      "ep: 114, time: 0.197, train_loss: 0.05251450659707189, train_acc: 1.0, test_loss: 0.054948021471500394, test_acc: 1.0\n",
      "ep: 115, time: 0.210, train_loss: 0.04935751101002097, train_acc: 1.0, test_loss: 0.051660549268126486, test_acc: 1.0\n",
      "ep: 116, time: 0.190, train_loss: 0.046495561767369506, train_acc: 1.0, test_loss: 0.04871712103486061, test_acc: 1.0\n",
      "ep: 117, time: 0.218, train_loss: 0.04388822773471475, train_acc: 1.0, test_loss: 0.046030768752098085, test_acc: 1.0\n",
      "ep: 118, time: 0.191, train_loss: 0.04150273473933339, train_acc: 1.0, test_loss: 0.04357055388391018, test_acc: 1.0\n",
      "ep: 119, time: 0.228, train_loss: 0.03931180159561336, train_acc: 1.0, test_loss: 0.041309894248843196, test_acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "model = RNN()\n",
    "train_model(model, 120, 20, (x_train, x_test), (y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test: tensor([9, 2, 9, 7, 7, 2, 8, 6, 4, 4, 4, 8, 5, 3, 6, 3, 4, 8, 9, 5])\n",
      "y_test: tensor([9, 1, 8, 6, 6, 1, 7, 5, 3, 3, 3, 7, 4, 2, 5, 2, 3, 7, 8, 4])\n",
      "y_pred: tensor([9, 1, 8, 6, 6, 1, 7, 5, 3, 3, 3, 7, 4, 2, 5, 2, 3, 7, 8, 4])\n",
      "=====\n",
      "x_test: tensor([2, 3, 7, 5, 0, 9, 6, 3, 1, 9, 5, 3, 5, 5, 3, 4, 6, 9, 6, 5])\n",
      "y_test: tensor([2, 5, 9, 7, 2, 1, 8, 5, 3, 1, 7, 5, 7, 7, 5, 6, 8, 1, 8, 7])\n",
      "y_pred: tensor([2, 5, 9, 7, 2, 1, 8, 5, 3, 1, 7, 5, 7, 7, 5, 6, 8, 1, 8, 7])\n",
      "=====\n",
      "x_test: tensor([4, 5, 3, 8, 8, 8, 8, 9, 3, 8, 9, 7, 6, 8, 0, 5, 8, 4, 0, 3])\n",
      "y_test: tensor([4, 9, 7, 2, 2, 2, 2, 3, 7, 2, 3, 1, 0, 2, 4, 9, 2, 8, 4, 7])\n",
      "y_pred: tensor([4, 9, 7, 2, 2, 2, 2, 3, 7, 2, 3, 1, 0, 2, 4, 9, 2, 8, 4, 7])\n",
      "=====\n",
      "x_test: tensor([7, 0, 9, 6, 0, 3, 1, 4, 3, 7, 6, 0, 9, 5, 4, 8, 2, 6, 3, 8])\n",
      "y_test: tensor([7, 7, 6, 3, 7, 0, 8, 1, 0, 4, 3, 7, 6, 2, 1, 5, 9, 3, 0, 5])\n",
      "y_pred: tensor([7, 7, 6, 3, 7, 0, 8, 1, 0, 4, 3, 7, 6, 2, 1, 5, 9, 3, 0, 5])\n",
      "=====\n",
      "x_test: tensor([0, 8, 3, 9, 0, 6, 0, 3, 2, 9, 7, 2, 8, 4, 8, 9, 8, 9, 0, 9])\n",
      "y_test: tensor([0, 8, 3, 9, 0, 6, 0, 3, 2, 9, 7, 2, 8, 4, 8, 9, 8, 9, 0, 9])\n",
      "y_pred: tensor([0, 8, 3, 9, 0, 6, 0, 3, 2, 9, 7, 2, 8, 4, 8, 9, 8, 9, 0, 9])\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'x_test: {x_test[i]}\\ny_test: {y_test[i]}\\ny_pred: {model(x_test[i]).argmax(dim=1)}\\n=====')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** как метрики, так и визуальная проверка показывают, что наша модель RNN справилась с поставленной задачей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRU(torch.nn.Module):\n",
    "    def __init__(self, rnnClass, dictionary_size, embedding_size, num_hiddens, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = torch.nn.Embedding(dictionary_size, embedding_size)\n",
    "        self.hidden = rnnClass(embedding_size, num_hiddens, batch_first=True)\n",
    "        self.output = torch.nn.Linear(num_hiddens, num_classes)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.embedding(X)\n",
    "        o, _ = self.hidden(out)\n",
    "        predictions = self.output(o)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, time: 0.573, train_loss: 2.3048239827156065, train_acc: 0.108875, test_loss: 2.2988078117370607, test_acc: 0.1215\n",
      "ep: 1, time: 0.577, train_loss: 2.296009248495102, train_acc: 0.1315, test_loss: 2.2978891849517824, test_acc: 0.1285\n",
      "ep: 2, time: 0.556, train_loss: 2.292725068330765, train_acc: 0.132875, test_loss: 2.297342896461487, test_acc: 0.1355\n",
      "ep: 3, time: 0.532, train_loss: 2.2898297905921936, train_acc: 0.1370625, test_loss: 2.2966761589050293, test_acc: 0.14025\n",
      "ep: 4, time: 0.530, train_loss: 2.2863627552986143, train_acc: 0.141125, test_loss: 2.2953736543655396, test_acc: 0.14075\n",
      "ep: 5, time: 0.552, train_loss: 2.2809235990047454, train_acc: 0.1485625, test_loss: 2.291536235809326, test_acc: 0.14925\n",
      "ep: 6, time: 0.515, train_loss: 2.2677515387535094, train_acc: 0.1610625, test_loss: 2.2668930768966673, test_acc: 0.1755\n",
      "ep: 7, time: 0.501, train_loss: 2.182557448744774, train_acc: 0.2201875, test_loss: 2.107844829559326, test_acc: 0.2715\n",
      "ep: 8, time: 0.525, train_loss: 1.9344570010900497, train_acc: 0.3288125, test_loss: 1.7794217586517334, test_acc: 0.4\n",
      "ep: 9, time: 0.506, train_loss: 1.548885667324066, train_acc: 0.5104375, test_loss: 1.355558943748474, test_acc: 0.591\n",
      "ep: 10, time: 0.544, train_loss: 1.088074505329132, train_acc: 0.732125, test_loss: 0.9246969103813172, test_acc: 0.8025\n",
      "ep: 11, time: 0.543, train_loss: 0.6737876959145069, train_acc: 0.9130625, test_loss: 0.5533876776695251, test_acc: 0.96\n",
      "ep: 12, time: 0.501, train_loss: 0.3866249047219753, train_acc: 0.987625, test_loss: 0.32736195921897887, test_acc: 0.9925\n",
      "ep: 13, time: 0.518, train_loss: 0.22493734434247017, train_acc: 0.99925, test_loss: 0.20442756414413452, test_acc: 0.9945\n",
      "ep: 14, time: 0.491, train_loss: 0.1418376861140132, train_acc: 1.0, test_loss: 0.14087993055582046, test_acc: 0.99525\n",
      "ep: 15, time: 0.521, train_loss: 0.09686260968446732, train_acc: 1.0, test_loss: 0.10118220821022987, test_acc: 0.99625\n",
      "ep: 16, time: 0.522, train_loss: 0.07069994192570447, train_acc: 1.0, test_loss: 0.07803228944540024, test_acc: 0.9975\n",
      "ep: 17, time: 0.555, train_loss: 0.05432656779885292, train_acc: 1.0, test_loss: 0.06205394938588142, test_acc: 0.998\n",
      "ep: 18, time: 0.516, train_loss: 0.043414550833404064, train_acc: 1.0, test_loss: 0.04395868070423603, test_acc: 1.0\n",
      "ep: 19, time: 0.568, train_loss: 0.03572976463474333, train_acc: 1.0, test_loss: 0.03650578651577234, test_acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "model = GRU(torch.nn.GRU, 10, 20, 128, 10)\n",
    "train_model(model, 20, 20, (x_train, x_test), (y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test: tensor([9, 2, 9, 7, 7, 2, 8, 6, 4, 4, 4, 8, 5, 3, 6, 3, 4, 8, 9, 5])\n",
      "y_test: tensor([9, 1, 8, 6, 6, 1, 7, 5, 3, 3, 3, 7, 4, 2, 5, 2, 3, 7, 8, 4])\n",
      "y_pred: tensor([9, 1, 8, 6, 6, 1, 7, 5, 3, 3, 3, 7, 4, 2, 5, 2, 3, 7, 8, 4])\n",
      "=====\n",
      "x_test: tensor([2, 3, 7, 5, 0, 9, 6, 3, 1, 9, 5, 3, 5, 5, 3, 4, 6, 9, 6, 5])\n",
      "y_test: tensor([2, 5, 9, 7, 2, 1, 8, 5, 3, 1, 7, 5, 7, 7, 5, 6, 8, 1, 8, 7])\n",
      "y_pred: tensor([2, 5, 9, 7, 2, 1, 8, 5, 3, 1, 7, 5, 7, 7, 5, 6, 8, 1, 8, 7])\n",
      "=====\n",
      "x_test: tensor([4, 5, 3, 8, 8, 8, 8, 9, 3, 8, 9, 7, 6, 8, 0, 5, 8, 4, 0, 3])\n",
      "y_test: tensor([4, 9, 7, 2, 2, 2, 2, 3, 7, 2, 3, 1, 0, 2, 4, 9, 2, 8, 4, 7])\n",
      "y_pred: tensor([4, 9, 7, 2, 2, 2, 2, 3, 7, 2, 3, 1, 0, 2, 4, 9, 2, 8, 4, 7])\n",
      "=====\n",
      "x_test: tensor([7, 0, 9, 6, 0, 3, 1, 4, 3, 7, 6, 0, 9, 5, 4, 8, 2, 6, 3, 8])\n",
      "y_test: tensor([7, 7, 6, 3, 7, 0, 8, 1, 0, 4, 3, 7, 6, 2, 1, 5, 9, 3, 0, 5])\n",
      "y_pred: tensor([7, 7, 6, 3, 7, 0, 8, 1, 0, 4, 3, 7, 6, 2, 1, 5, 9, 3, 0, 5])\n",
      "=====\n",
      "x_test: tensor([0, 8, 3, 9, 0, 6, 0, 3, 2, 9, 7, 2, 8, 4, 8, 9, 8, 9, 0, 9])\n",
      "y_test: tensor([0, 8, 3, 9, 0, 6, 0, 3, 2, 9, 7, 2, 8, 4, 8, 9, 8, 9, 0, 9])\n",
      "y_pred: tensor([0, 8, 3, 9, 0, 6, 0, 3, 2, 9, 7, 2, 8, 4, 8, 9, 8, 9, 0, 9])\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'x_test: {x_test[i]}\\ny_test: {y_test[i]}\\ny_pred: {model(x_test[i]).argmax(dim=1)}\\n=====')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** Модель GRU сошлась за значительно меньшее количество эпох, нежели простая RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(torch.nn.Module):\n",
    "    def __init__(self, rnnClass, dictionary_size, embedding_size, num_hiddens, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.num_hiddens = num_hiddens\n",
    "        self.embedding = torch.nn.Embedding(dictionary_size, embedding_size)\n",
    "        self.hidden = rnnClass(embedding_size, num_hiddens, batch_first=True)\n",
    "        self.output = torch.nn.Linear(num_hiddens, num_classes)\n",
    "        \n",
    "    def forward(self, X):\n",
    "        out = self.embedding(X)\n",
    "        o, *_ = self.hidden(out)\n",
    "        predictions = self.output(o)\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep: 0, time: 0.553, train_loss: 2.303363800048828, train_acc: 0.112875, test_loss: 2.2988298654556276, test_acc: 0.131\n",
      "ep: 1, time: 0.616, train_loss: 2.2960260272026063, train_acc: 0.1323125, test_loss: 2.296048808097839, test_acc: 0.14\n",
      "ep: 2, time: 0.588, train_loss: 2.291681706905365, train_acc: 0.142125, test_loss: 2.294112968444824, test_acc: 0.14275\n",
      "ep: 3, time: 0.635, train_loss: 2.2857984244823455, train_acc: 0.1466875, test_loss: 2.2893948554992676, test_acc: 0.14575\n",
      "ep: 4, time: 0.642, train_loss: 2.25736209154129, train_acc: 0.18025, test_loss: 2.2266605854034425, test_acc: 0.2075\n",
      "ep: 5, time: 0.603, train_loss: 2.174029541015625, train_acc: 0.22125, test_loss: 2.133845663070679, test_acc: 0.2275\n",
      "ep: 6, time: 0.596, train_loss: 2.07178615629673, train_acc: 0.244875, test_loss: 2.0395862340927122, test_acc: 0.256\n",
      "ep: 7, time: 0.629, train_loss: 1.9723564386367798, train_acc: 0.2651875, test_loss: 1.929352045059204, test_acc: 0.294\n",
      "ep: 8, time: 0.643, train_loss: 1.8792552143335342, train_acc: 0.325125, test_loss: 1.8307955145835877, test_acc: 0.35725\n",
      "ep: 9, time: 0.630, train_loss: 1.7499678909778595, train_acc: 0.3954375, test_loss: 1.6898282408714294, test_acc: 0.43425\n",
      "ep: 10, time: 0.648, train_loss: 1.5872951060533524, train_acc: 0.4773125, test_loss: 1.5040363669395447, test_acc: 0.52525\n",
      "ep: 11, time: 0.598, train_loss: 1.3814548432826996, train_acc: 0.5789375, test_loss: 1.2759643077850342, test_acc: 0.6325\n",
      "ep: 12, time: 0.600, train_loss: 1.1404134199023246, train_acc: 0.6964375, test_loss: 1.0168857157230378, test_acc: 0.74075\n",
      "ep: 13, time: 0.597, train_loss: 0.8675936669111252, train_acc: 0.827375, test_loss: 0.7400619208812713, test_acc: 0.88475\n",
      "ep: 14, time: 0.666, train_loss: 0.6152527093887329, train_acc: 0.935, test_loss: 0.5159341543912888, test_acc: 0.96225\n",
      "ep: 15, time: 0.678, train_loss: 0.4139812313020229, train_acc: 0.9813125, test_loss: 0.34558345675468444, test_acc: 0.98725\n",
      "ep: 16, time: 0.527, train_loss: 0.27747465297579765, train_acc: 0.9949375, test_loss: 0.2367337614297867, test_acc: 0.99625\n",
      "ep: 17, time: 0.513, train_loss: 0.19155346639454365, train_acc: 0.9985, test_loss: 0.1682976335287094, test_acc: 0.99825\n",
      "ep: 18, time: 0.503, train_loss: 0.1382280008867383, train_acc: 0.9996875, test_loss: 0.12501626685261727, test_acc: 0.99925\n",
      "ep: 19, time: 0.553, train_loss: 0.10391224473714829, train_acc: 0.9998125, test_loss: 0.09633962735533715, test_acc: 0.99925\n",
      "ep: 20, time: 0.510, train_loss: 0.08078449983149767, train_acc: 1.0, test_loss: 0.07645299360156059, test_acc: 1.0\n",
      "ep: 21, time: 0.539, train_loss: 0.0645501509308815, train_acc: 1.0, test_loss: 0.062072932720184326, test_acc: 1.0\n",
      "ep: 22, time: 0.505, train_loss: 0.05277012148872018, train_acc: 1.0, test_loss: 0.05139706209301949, test_acc: 1.0\n",
      "ep: 23, time: 0.560, train_loss: 0.04403098849579692, train_acc: 1.0, test_loss: 0.043341730162501334, test_acc: 1.0\n",
      "ep: 24, time: 0.563, train_loss: 0.037414749059826134, train_acc: 1.0, test_loss: 0.03716773428022861, test_acc: 1.0\n",
      "ep: 25, time: 0.607, train_loss: 0.03227269686758518, train_acc: 1.0, test_loss: 0.032300616055727004, test_acc: 1.0\n",
      "ep: 26, time: 0.556, train_loss: 0.02817650055512786, train_acc: 1.0, test_loss: 0.028374341689050196, test_acc: 1.0\n",
      "ep: 27, time: 0.551, train_loss: 0.024846811220049857, train_acc: 1.0, test_loss: 0.025150673650205135, test_acc: 1.0\n",
      "ep: 28, time: 0.574, train_loss: 0.022094675432890655, train_acc: 1.0, test_loss: 0.02246461622416973, test_acc: 1.0\n",
      "ep: 29, time: 0.515, train_loss: 0.019787825597450136, train_acc: 1.0, test_loss: 0.020198096893727778, test_acc: 1.0\n",
      "ep: 30, time: 0.535, train_loss: 0.017831216868944465, train_acc: 1.0, test_loss: 0.018264728412032128, test_acc: 1.0\n",
      "ep: 31, time: 0.531, train_loss: 0.01615490447729826, train_acc: 1.0, test_loss: 0.01660015555098653, test_acc: 1.0\n",
      "ep: 32, time: 0.621, train_loss: 0.014706370863132178, train_acc: 1.0, test_loss: 0.015155646111816167, test_acc: 1.0\n",
      "ep: 33, time: 0.593, train_loss: 0.013445552880875766, train_acc: 1.0, test_loss: 0.013893782906234265, test_acc: 1.0\n",
      "ep: 34, time: 0.571, train_loss: 0.01234129830263555, train_acc: 1.0, test_loss: 0.012785101775079966, test_acc: 1.0\n",
      "ep: 35, time: 0.467, train_loss: 0.01136863618157804, train_acc: 1.0, test_loss: 0.011805534549057483, test_acc: 1.0\n",
      "ep: 36, time: 0.564, train_loss: 0.010506944195367397, train_acc: 1.0, test_loss: 0.010935026686638594, test_acc: 1.0\n",
      "ep: 37, time: 0.595, train_loss: 0.009739048185292632, train_acc: 1.0, test_loss: 0.010156905557960273, test_acc: 1.0\n",
      "ep: 38, time: 0.555, train_loss: 0.009050701395608485, train_acc: 1.0, test_loss: 0.009457392804324626, test_acc: 1.0\n",
      "ep: 39, time: 0.703, train_loss: 0.00843018307350576, train_acc: 1.0, test_loss: 0.008825205592438578, test_acc: 1.0\n"
     ]
    }
   ],
   "source": [
    "model = LSTM(torch.nn.LSTM, 10, 20, 128, 10)\n",
    "train_model(model, 40, 20, (x_train, x_test), (y_train, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_test: tensor([9, 2, 9, 7, 7, 2, 8, 6, 4, 4, 4, 8, 5, 3, 6, 3, 4, 8, 9, 5])\n",
      "y_test: tensor([9, 1, 8, 6, 6, 1, 7, 5, 3, 3, 3, 7, 4, 2, 5, 2, 3, 7, 8, 4])\n",
      "y_pred: tensor([9, 1, 8, 6, 6, 1, 7, 5, 3, 3, 3, 7, 4, 2, 5, 2, 3, 7, 8, 4])\n",
      "=====\n",
      "x_test: tensor([2, 3, 7, 5, 0, 9, 6, 3, 1, 9, 5, 3, 5, 5, 3, 4, 6, 9, 6, 5])\n",
      "y_test: tensor([2, 5, 9, 7, 2, 1, 8, 5, 3, 1, 7, 5, 7, 7, 5, 6, 8, 1, 8, 7])\n",
      "y_pred: tensor([2, 5, 9, 7, 2, 1, 8, 5, 3, 1, 7, 5, 7, 7, 5, 6, 8, 1, 8, 7])\n",
      "=====\n",
      "x_test: tensor([4, 5, 3, 8, 8, 8, 8, 9, 3, 8, 9, 7, 6, 8, 0, 5, 8, 4, 0, 3])\n",
      "y_test: tensor([4, 9, 7, 2, 2, 2, 2, 3, 7, 2, 3, 1, 0, 2, 4, 9, 2, 8, 4, 7])\n",
      "y_pred: tensor([4, 9, 7, 2, 2, 2, 2, 3, 7, 2, 3, 1, 0, 2, 4, 9, 2, 8, 4, 7])\n",
      "=====\n",
      "x_test: tensor([7, 0, 9, 6, 0, 3, 1, 4, 3, 7, 6, 0, 9, 5, 4, 8, 2, 6, 3, 8])\n",
      "y_test: tensor([7, 7, 6, 3, 7, 0, 8, 1, 0, 4, 3, 7, 6, 2, 1, 5, 9, 3, 0, 5])\n",
      "y_pred: tensor([7, 7, 6, 3, 7, 0, 8, 1, 0, 4, 3, 7, 6, 2, 1, 5, 9, 3, 0, 5])\n",
      "=====\n",
      "x_test: tensor([0, 8, 3, 9, 0, 6, 0, 3, 2, 9, 7, 2, 8, 4, 8, 9, 8, 9, 0, 9])\n",
      "y_test: tensor([0, 8, 3, 9, 0, 6, 0, 3, 2, 9, 7, 2, 8, 4, 8, 9, 8, 9, 0, 9])\n",
      "y_pred: tensor([0, 8, 3, 9, 0, 6, 0, 3, 2, 9, 7, 2, 8, 4, 8, 9, 8, 9, 0, 9])\n",
      "=====\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    print(f'x_test: {x_test[i]}\\ny_test: {y_test[i]}\\ny_pred: {model(x_test[i]).argmax(dim=1)}\\n=====')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Вывод:** Модель LSTM сошлась за значительно меньшее количество эпох, нежели простая RNN, но чуть большее, чем GRU."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5a00e7ec61eff5a88fc1f6f522f8352e0b0489450640f58684f8d2503099540c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
